# --- B·∫ÆT BU·ªòC: FIX L·ªñI SQLITE TR√äN STREAMLIT CLOUD ---
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# -----------------------------------------------------

import streamlit as st
import json
import os
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# ================= C·∫§U H√åNH TRANG =================
st.set_page_config(page_title="Chatbot Th·ªß T·ª•c C∆∞ Tr√∫", layout="wide")
st.title("ü§ñ Chatbot T∆∞ V·∫•n Th·ªß T·ª•c C∆∞ Tr√∫ (RAG)")

# ================= C·∫§U H√åNH API & DATA =================
# L·∫•y API Key t·ª´ Secrets c·ªßa Streamlit
try:
    api_key = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=api_key)
except Exception:
    st.error("Ch∆∞a c·∫•u h√¨nh GEMINI_API_KEY trong Secrets!")
    st.stop()

# ƒê∆∞·ªùng d·∫´n file data (b·∫°n ph·∫£i upload file n√†y v√†o th∆∞ m·ª•c data tr√™n github)
JSON_FILE = "data/all_procedures_normalized.json"
COLLECTION_NAME = "dichvucong_rag_collection"

# ... (Ph·∫ßn import gi·ªØ nguy√™n nh∆∞ c≈©, nh·ªõ 3 d√≤ng fix sqlite ·ªü ƒë·∫ßu) ...

# ================= 3. H√ÄM LOAD D·ªÆ LI·ªÜU (C√ì B√ÅO L·ªñI CHI TI·∫æT) =================
@st.cache_resource(ttl="2h") 
def load_all_json_files():
    EMBEDDING_MODEL = "keepitreal/vietnamese-sbert"
    
    try:
        # 1. Kh·ªüi t·∫°o ChromaDB
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=EMBEDDING_MODEL
        )
        chroma_client = chromadb.Client()
        collection = chroma_client.get_or_create_collection(
            name=COLLECTION_NAME,
            embedding_function=embedding_function
        )
        
        # 2. T√¨m file JSON
        json_files = glob.glob("*.json")
        if not json_files:
            return "NO_FILES_FOUND" # M√£ l·ªói ri√™ng

        # 3. ƒê·ªçc d·ªØ li·ªáu
        if collection.count() == 0:
            ids, documents, metadatas = [], [], []
            
            for file_name in json_files:
                with open(file_name, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    for item in data:
                        # L·ªçc d·ªØ li·ªáu r·ªóng
                        if not item.get("content_text"): continue
                        
                        ids.append(str(uuid.uuid4()))
                        documents.append(item.get("content_text"))
                        
                        # X·ª≠ l√Ω metadata
                        meta = item.get("metadata", {}).copy()
                        meta["source_file"] = file_name
                        # X√≥a gi√° tr·ªã None ƒë·ªÉ tr√°nh l·ªói
                        clean_meta = {k: str(v) for k, v in meta.items() if v is not None}
                        metadatas.append(clean_meta)
            
            if not documents:
                return "EMPTY_DATA"

            # 4. N·∫°p Batch
            batch_size = 40
            for i in range(0, len(ids), batch_size):
                collection.add(
                    ids=ids[i:i+batch_size],
                    documents=documents[i:i+batch_size],
                    metadatas=metadatas[i:i+batch_size]
                )
                
        return collection
        
    except Exception as e:
        # TR·∫¢ V·ªÄ CHI TI·∫æT L·ªñI ƒê·ªÇ DEBUG
        return f"ERROR_DETAIL: {str(e)}"

# --- G·ªåI H√ÄM (S·ª¨A L·∫†I ƒê·ªÇ B·∫ÆT L·ªñI) ---
with st.spinner("ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng..."):
    collection = load_all_json_files()

if isinstance(collection, str): # N·∫øu tr·∫£ v·ªÅ chu·ªói nghƒ©a l√† c√≥ l·ªói
    if "ERROR_DETAIL" in collection:
        st.error(f"‚ùå L·ªñI H·ªÜ TH·ªêNG CHI TI·∫æT: {collection}")
        st.info("üëâ H√£y ch·ª•p ·∫£nh l·ªói n√†y g·ª≠i cho t√¥i ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£!")
    elif collection == "NO_FILES_FOUND":
        st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file .json n√†o tr√™n GitHub. B·∫°n ƒë√£ upload file ch∆∞a?")
    st.stop()

# N·∫øu th√†nh c√¥ng
st.sidebar.success(f"‚úÖ ƒê√£ n·∫°p: **{collection.count()}** chunks")

# ================= H√ÄM TRUY V·∫§N (RAG) =================
def query_gemini(question, collection, model_name="gemini-2.5-flash"):
    # 1. Truy v·∫•n Vector DB
    results = collection.query(
        query_texts=[question],
        n_results=5, # L·∫•y 5 ƒëo·∫°n li√™n quan nh·∫•t
        include=["documents", "metadatas"]
    )

    # 2. X√¢y d·ª±ng Context
    context_parts = []
    sources = []
    
    if results["documents"]:
        for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
            hierarchy = meta.get('hierarchy', 'Th√¥ng tin')
            url = meta.get('url', '#')
            context_parts.append(f"[{hierarchy}]\n{doc}")
            sources.append(f"- [{hierarchy}]({url})")

    context = "\n\n".join(context_parts)

    if not context:
        return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong d·ªØ li·ªáu c·ªßa m√¨nh.", []

    # 3. T·∫°o Prompt
    prompt = f"""
    B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh c√¥ng c·ªßa Vi·ªát Nam (lƒ©nh v·ª±c C∆∞ tr√∫).
    
    NGUY√äN T·∫ÆC:
    - Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong CONTEXT b√™n d∆∞·ªõi.
    - Kh√¥ng b·ªãa ƒë·∫∑t th√¥ng tin. N·∫øu kh√¥ng c√≥ trong context, h√£y n√≥i kh√¥ng bi·∫øt.
    - Tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng, ƒë√°nh s·ªë b∆∞·ªõc n·∫øu c·∫ßn.
    
    CONTEXT:
    {context}
    
    C√ÇU H·ªéI: {question}
    """

    # 4. G·ªçi Gemini
    try:
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(prompt)
        return response.text, list(set(sources)) # Tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi v√† ngu·ªìn (unique)
    except Exception as e:
        return f"L·ªói k·∫øt n·ªëi Gemini: {str(e)}", []

# ================= GIAO DI·ªÜN CH√çNH =================
# Load Database
collection = load_vector_db()

if collection:
    st.sidebar.success(f"D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng: {collection.count()} chunks")
else:
    st.stop()

# Kh·ªüi t·∫°o l·ªãch s·ª≠ chat
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ v·ªÅ th·ªß t·ª•c Th∆∞·ªùng tr√∫, T·∫°m tr√∫, T√°ch h·ªô...?"}]

# Hi·ªÉn th·ªã l·ªãch s·ª≠
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# X·ª≠ l√Ω input ng∆∞·ªùi d√πng
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    # Hi·ªÉn th·ªã c√¢u h·ªèi user
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # X·ª≠ l√Ω c√¢u tr·∫£ l·ªùi
    with st.chat_message("assistant"):
        with st.spinner("ƒêang tra c·ª©u lu·∫≠t..."):
            response_text, sources = query_gemini(prompt, collection)
            
            # Format c√¢u tr·∫£ l·ªùi k√®m ngu·ªìn
            final_content = response_text
            if sources:
                final_content += "\n\n**Ngu·ªìn tham kh·∫£o:**\n" + "\n".join(sources)
            
            st.markdown(final_content)
            st.session_state.messages.append({"role": "assistant", "content": final_content})
