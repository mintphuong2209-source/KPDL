# ================= 1. FIX L·ªñI SQLITE (B·∫ÆT BU·ªòC) =================
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# ===============================================================

import streamlit as st
import json
import os
import glob
import time
import uuid
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai
from google.api_core import exceptions

# ================= C·∫§U H√åNH TRANG =================
st.set_page_config(page_title="Chatbot Ph√°p Lu·∫≠t", layout="wide")
st.title("ü§ñ Chatbot T∆∞ V·∫•n (Ch·∫ø ƒë·ªô Fallback Th√¥ng Minh)")

COLLECTION_NAME = "dichvucong_smart_v1"

# ================= 2. C·∫§U H√åNH API =================
st.sidebar.header("‚öôÔ∏è C·∫•u h√¨nh")
api_key = st.secrets.get("GEMINI_API_KEY")
if not api_key:
    api_key = st.sidebar.text_input("Nh·∫≠p Google AI API Key:", type="password")
    if not api_key:
        st.warning("üëâ Vui l√≤ng nh·∫≠p API Key.")
        st.stop()

genai.configure(api_key=api_key)

# --- DANH S√ÅCH MODEL ∆ØU TI√äN (Theo th·ª© t·ª±) ---
# H·ªá th·ªëng s·∫Ω th·ª≠ l·∫ßn l∆∞·ª£t t·ª´ tr√™n xu·ªëng d∆∞·ªõi
PRIORITY_MODELS = [
    "gemini-1.5-flash",          # ∆Øu ti√™n 1: Nhanh, r·∫ª
    "gemini-1.5-pro",            # ∆Øu ti√™n 2: Th√¥ng minh h∆°n (n·∫øu Flash l·ªói)
    "gemini-1.0-pro",            # ∆Øu ti√™n 3: B·∫£n c≈© ·ªïn ƒë·ªãnh
    "gemini-1.5-flash-latest"    # ∆Øu ti√™n 4: B·∫£n m·ªõi nh·∫•t
]

# ================= 3. H√ÄM G·ªåI AI TH√îNG MINH (FALLBACK + RETRY) =================
def call_smart_ai(prompt):
    """
    H√†m n√†y t·ª± ƒë·ªông th·ª≠ c√°c model kh√°c nhau cho ƒë·∫øn khi th√†nh c√¥ng.
    """
    debug_logs = []
    
    for model_name in PRIORITY_MODELS:
        # V·ªõi m·ªói model, th·ª≠ t·ªëi ƒëa 2 l·∫ßn n·∫øu m·∫°ng ch·∫≠p ch·ªùn
        for attempt in range(2):
            try:
                # T·∫°o model
                model = genai.GenerativeModel(model_name)
                
                # G·ªçi AI
                response = model.generate_content(prompt)
                
                # N·∫øu th√†nh c√¥ng -> Tr·∫£ v·ªÅ ngay
                return response.text, f"‚úÖ ƒê√£ tr·∫£ l·ªùi b·∫±ng: **{model_name}**"
                
            except Exception as e:
                error_msg = str(e)
                # N·∫øu l·ªói quota (h·∫øt ti·ªÅn/h·∫øt l∆∞·ª£t) -> B·ªè qua model n√†y ngay l·∫≠p t·ª©c
                if "429" in error_msg or "ResourceExhausted" in error_msg:
                    debug_logs.append(f"‚ö†Ô∏è {model_name}: Qu√° t·∫£i (Quota exceeded).")
                    break # Tho√°t v√≤ng l·∫∑p retry, chuy·ªÉn sang model k·∫ø ti·∫øp
                
                # N·∫øu l·ªói kh√°c -> Th·ª≠ l·∫°i 1 l·∫ßn n·ªØa
                debug_logs.append(f"‚ö†Ô∏è {model_name} (L·∫ßn {attempt+1}): {error_msg}")
                time.sleep(1) # Ngh·ªâ 1s
    
    # N·∫øu th·ª≠ h·∫øt t·∫•t c·∫£ m√† v·∫´n t·∫°ch
    return None, "\n".join(debug_logs)

# ================= 4. H√ÄM N·∫†P D·ªÆ LI·ªÜU =================
@st.cache_resource(ttl="2h")
def load_database():
    try:
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="keepitreal/vietnamese-sbert"
        )
        client = chromadb.Client()
        collection = client.get_or_create_collection(
            name=COLLECTION_NAME,
            embedding_function=embedding_function
        )

        if collection.count() == 0:
            json_files = glob.glob("*.json")
            if not json_files: return None, "NO_FILES"

            all_ids, all_docs, all_metas = [], [], []
            for file_path in json_files:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        for item in data:
                            content = item.get("content_text", "").strip()
                            if len(content) < 10: continue
                            all_ids.append(str(uuid.uuid4()))
                            all_docs.append(content)
                            meta = item.get("metadata", {}).copy()
                            meta["url"] = item.get("url", "#")
                            clean_meta = {k: str(v) for k, v in meta.items() if v is not None}
                            all_metas.append(clean_meta)

            if not all_docs: return None, "EMPTY"

            batch_size = 40
            for i in range(0, len(all_ids), batch_size):
                collection.add(
                    ids=all_ids[i:i+batch_size],
                    documents=all_docs[i:i+batch_size],
                    metadatas=all_metas[i:i+batch_size]
                )
        return collection, "OK"
    except Exception as e:
        return None, str(e)

# --- KH·ªûI ƒê·ªòNG ---
with st.spinner("ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng..."):
    collection, status = load_database()

if collection:
    st.sidebar.success(f"‚úÖ D·ªØ li·ªáu s·∫µn s√†ng: {collection.count()} chunks")
else:
    st.error(f"L·ªói: {status}")
    st.stop()

# ================= 5. X·ª¨ L√ù C√ÇU H·ªéI =================
def query_system(question):
    # 1. T√¨m ki·∫øm d·ªØ li·ªáu
    results = collection.query(query_texts=[question], n_results=5)
    
    context = ""
    links = []
    if results['documents']:
        for i, doc in enumerate(results['documents'][0]):
            meta = results['metadatas'][0][i]
            url = meta.get('url', '#')
            context += f"- {doc}\n"
            if url != '#': links.append(url)
    
    if not context:
        return "Kh√¥ng t√¨m th·∫•y th√¥ng tin trong d·ªØ li·ªáu.", "", []

    # 2. T·∫°o Prompt
    prompt = f"""
    B·∫°n l√† tr·ª£ l√Ω ·∫£o ph√°p lu·∫≠t. D·ª±a v√†o th√¥ng tin sau:\n{context}
    \nH√£y tr·∫£ l·ªùi c√¢u h·ªèi: {question}
    """
    
    # 3. G·ªçi h√†m th√¥ng minh (Fallback)
    answer, log_info = call_smart_ai(prompt)
    
    if answer:
        return answer, log_info, list(set(links))
    else:
        return f"Xin l·ªói, t·∫•t c·∫£ c√°c h·ªá th·ªëng AI ƒë·ªÅu ƒëang b·∫≠n.\nChi ti·∫øt l·ªói:\n{log_info}", "", []

# ================= 6. GIAO DI·ªÜN CHAT =================
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Xin ch√†o! B·∫°n c·∫ßn h·ªèi g√¨?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    
    with st.chat_message("assistant"):
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            ans, debug_info, sources = query_system(prompt)
            
            st.markdown(ans)
            
            # Hi·ªÉn th·ªã model n√†o ƒë√£ tr·∫£ l·ªùi (ƒë·ªÉ b·∫°n bi·∫øt)
            if debug_info:
                st.caption(debug_info)
                
            if sources:
                st.markdown("**Ngu·ªìn tham kh·∫£o:**")
                for s in sources: st.markdown(f"- {s}")
            
            st.session_state.messages.append({"role": "assistant", "content": ans})
