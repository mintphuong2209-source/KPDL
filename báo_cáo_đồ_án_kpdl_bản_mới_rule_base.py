# --- B·∫ÆT BU·ªòC: FIX L·ªñI SQLITE TR√äN STREAMLIT CLOUD ---
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# -----------------------------------------------------

import streamlit as st
import json
import os
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# ================= C·∫§U H√åNH TRANG =================
st.set_page_config(page_title="Chatbot Th·ªß T·ª•c C∆∞ Tr√∫", layout="wide")
st.title("ü§ñ Chatbot T∆∞ V·∫•n Th·ªß T·ª•c C∆∞ Tr√∫ (RAG)")

# ================= C·∫§U H√åNH API & DATA =================
# L·∫•y API Key t·ª´ Secrets c·ªßa Streamlit
try:
    api_key = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=api_key)
except Exception:
    st.error("Ch∆∞a c·∫•u h√¨nh GEMINI_API_KEY trong Secrets!")
    st.stop()

# ƒê∆∞·ªùng d·∫´n file data (b·∫°n ph·∫£i upload file n√†y v√†o th∆∞ m·ª•c data tr√™n github)
JSON_FILE = "data/all_procedures_normalized.json"
COLLECTION_NAME = "dichvucong_rag_collection"

# ================= H√ÄM LOAD D·ªÆ LI·ªÜU (CACHED) =================
@st.cache_resource
def load_vector_db():
    """
    Kh·ªüi t·∫°o ChromaDB v√† n·∫°p d·ªØ li·ªáu t·ª´ file JSON.
    D√πng cache ƒë·ªÉ kh√¥ng ph·∫£i n·∫°p l·∫°i m·ªói l·∫ßn reload trang.
    """
    # S·ª≠ d·ª•ng model nh·∫π h∆°n bge-m3 m·ªôt ch√∫t ƒë·ªÉ ch·∫°y m∆∞·ª£t tr√™n Cloud Free
    # Ho·∫∑c b·∫°n c√≥ th·ªÉ gi·ªØ nguy√™n "BAAI/bge-m3" n·∫øu mu·ªën
    EMBEDDING_MODEL = "keepitreal/vietnamese-sbert" 
    
    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=EMBEDDING_MODEL
    )

    # S·ª≠ d·ª•ng EphemeralClient (ch·∫°y tr√™n RAM) cho m√¥i tr∆∞·ªùng Cloud
    chroma_client = chromadb.Client()
    
    # T·∫°o ho·∫∑c l·∫•y collection
    collection = chroma_client.get_or_create_collection(
        name=COLLECTION_NAME,
        embedding_function=embedding_function
    )

    # Ki·ªÉm tra n·∫øu collection r·ªóng th√¨ m·ªõi n·∫°p
    if collection.count() == 0:
        if not os.path.exists(JSON_FILE):
            st.error(f"Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu t·∫°i {JSON_FILE}")
            return None

        with open(JSON_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)

        ids = []
        documents = []
        metadatas = []

        # Chu·∫©n b·ªã d·ªØ li·ªáu (Batch processing ƒë·ªÉ tr√°nh qu√° t·∫£i RAM)
        batch_size = 100
        total_chunks = len(data)
        
        progress_text = "ƒêang n·∫°p d·ªØ li·ªáu v√†o b·ªô nh·ªõ..."
        my_bar = st.progress(0, text=progress_text)

        for idx, item in enumerate(data):
            ids.append(str(item.get("id", idx))) # ƒê·∫£m b·∫£o ID l√† string
            documents.append(item["content_text"])
            
            # X·ª≠ l√Ω metadata (Chroma kh√¥ng nh·∫≠n None value, ph·∫£i chuy·ªÉn th√†nh string r·ªóng)
            meta = item.get("metadata", {}).copy()
            meta.update({
                "url": item.get("url", ""),
                "title": item.get("title", ""),
                "hierarchy": item.get("hierarchy", ""),
                "chunk_type": item.get("chunk_type", ""),
            })
            # Clean metadata values
            clean_meta = {k: str(v) if v is not None else "" for k, v in meta.items()}
            metadatas.append(clean_meta)

        # Add to Chroma theo batch
        for i in range(0, len(ids), batch_size):
            end_idx = min(i + batch_size, len(ids))
            collection.add(
                ids=ids[i:end_idx],
                documents=documents[i:end_idx],
                metadatas=metadatas[i:end_idx]
            )
            my_bar.progress(min(i / total_chunks, 1.0), text=f"ƒê√£ n·∫°p {i}/{total_chunks} chunks")
        
        my_bar.empty()
        
    return collection

# ================= H√ÄM TRUY V·∫§N (RAG) =================
def query_gemini(question, collection, model_name="gemini-2.5-flash"):
    # 1. Truy v·∫•n Vector DB
    results = collection.query(
        query_texts=[question],
        n_results=5, # L·∫•y 5 ƒëo·∫°n li√™n quan nh·∫•t
        include=["documents", "metadatas"]
    )

    # 2. X√¢y d·ª±ng Context
    context_parts = []
    sources = []
    
    if results["documents"]:
        for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
            hierarchy = meta.get('hierarchy', 'Th√¥ng tin')
            url = meta.get('url', '#')
            context_parts.append(f"[{hierarchy}]\n{doc}")
            sources.append(f"- [{hierarchy}]({url})")

    context = "\n\n".join(context_parts)

    if not context:
        return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong d·ªØ li·ªáu c·ªßa m√¨nh.", []

    # 3. T·∫°o Prompt
    prompt = f"""
    B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh c√¥ng c·ªßa Vi·ªát Nam (lƒ©nh v·ª±c C∆∞ tr√∫).
    
    NGUY√äN T·∫ÆC:
    - Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong CONTEXT b√™n d∆∞·ªõi.
    - Kh√¥ng b·ªãa ƒë·∫∑t th√¥ng tin. N·∫øu kh√¥ng c√≥ trong context, h√£y n√≥i kh√¥ng bi·∫øt.
    - Tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng, ƒë√°nh s·ªë b∆∞·ªõc n·∫øu c·∫ßn.
    
    CONTEXT:
    {context}
    
    C√ÇU H·ªéI: {question}
    """

    # 4. G·ªçi Gemini
    try:
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(prompt)
        return response.text, list(set(sources)) # Tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi v√† ngu·ªìn (unique)
    except Exception as e:
        return f"L·ªói k·∫øt n·ªëi Gemini: {str(e)}", []

# ================= GIAO DI·ªÜN CH√çNH =================
# Load Database
collection = load_vector_db()

if collection:
    st.sidebar.success(f"D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng: {collection.count()} chunks")
else:
    st.stop()

# Kh·ªüi t·∫°o l·ªãch s·ª≠ chat
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ v·ªÅ th·ªß t·ª•c Th∆∞·ªùng tr√∫, T·∫°m tr√∫, T√°ch h·ªô...?"}]

# Hi·ªÉn th·ªã l·ªãch s·ª≠
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# X·ª≠ l√Ω input ng∆∞·ªùi d√πng
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    # Hi·ªÉn th·ªã c√¢u h·ªèi user
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # X·ª≠ l√Ω c√¢u tr·∫£ l·ªùi
    with st.chat_message("assistant"):
        with st.spinner("ƒêang tra c·ª©u lu·∫≠t..."):
            response_text, sources = query_gemini(prompt, collection)
            
            # Format c√¢u tr·∫£ l·ªùi k√®m ngu·ªìn
            final_content = response_text
            if sources:
                final_content += "\n\n**Ngu·ªìn tham kh·∫£o:**\n" + "\n".join(sources)
            
            st.markdown(final_content)
            st.session_state.messages.append({"role": "assistant", "content": final_content})
