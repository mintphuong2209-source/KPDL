# ================= 1. FIX L·ªñI SQLITE TR√äN CLOUD (B·∫ÆT BU·ªòC) =================
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# ===========================================================================

import streamlit as st
import json
import os
import uuid
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# ================= C·∫§U H√åNH TRANG =================
st.set_page_config(page_title="H·ªèi ƒê√°p Th·ªß T·ª•c C∆∞ Tr√∫", layout="wide")
st.title("ü§ñ Chatbot T∆∞ V·∫•n Th·ªß T·ª•c C∆∞ Tr√∫ (D·ªØ li·ªáu BCA)")

# T√™n file d·ªØ li·ªáu (ph·∫£i kh·ªõp v·ªõi t√™n file b·∫°n upload l√™n GitHub)
JSON_FILE = "all_chunks_normalized.json"
COLLECTION_NAME = "dichvucong_data_final"

# ================= 2. C·∫§U H√åNH API =================
st.sidebar.header("‚öôÔ∏è C·∫•u h√¨nh")

# L·∫•y API Key t·ª´ Secrets (∆Øu ti√™n) ho·∫∑c nh·∫≠p tay
api_key = st.secrets.get("GEMINI_API_KEY")
if not api_key:
    api_key = st.sidebar.text_input("Nh·∫≠p Google AI API Key:", type="password")
    if not api_key:
        st.warning("üëâ Vui l√≤ng nh·∫≠p API Key ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
        st.stop()

genai.configure(api_key=api_key)

# ================= 3. H√ÄM N·∫†P D·ªÆ LI·ªÜU (CORE LOGIC) =================
# H√†m n√†y ch·ªâ ch·∫°y 1 l·∫ßn duy nh·∫•t khi kh·ªüi ƒë·ªông app
@st.cache_resource(show_spinner=False)
def initialize_database():
    """
    H√†m kh·ªüi t·∫°o Vector DB v√† n·∫°p d·ªØ li·ªáu t·ª´ file JSON.
    Tuy·ªát ƒë·ªëi KH√îNG d√πng st.write, st.spinner ·ªü trong h√†m n√†y ƒë·ªÉ tr√°nh l·ªói Cache.
    """
    try:
        # 1. C·∫•u h√¨nh ChromaDB v·ªõi model nh√∫ng ti·∫øng Vi·ªát
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="keepitreal/vietnamese-sbert"
        )
        chroma_client = chromadb.Client()
        collection = chroma_client.get_or_create_collection(
            name=COLLECTION_NAME,
            embedding_function=embedding_function
        )

        # 2. Ki·ªÉm tra n·∫øu DB ch∆∞a c√≥ d·ªØ li·ªáu th√¨ m·ªõi n·∫°p
        if collection.count() == 0:
            if not os.path.exists(JSON_FILE):
                return None, f"FILE_NOT_FOUND: Kh√¥ng t√¨m th·∫•y file {JSON_FILE}"

            with open(JSON_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)

            if not data or not isinstance(data, list):
                return None, "INVALID_DATA: File JSON l·ªói ho·∫∑c r·ªóng"

            # 3. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ n·∫°p
            ids = []
            documents = []
            metadatas = []

            for idx, item in enumerate(data):
                # L·∫•y n·ªôi dung
                content = item.get("content_text", "").strip()
                if not content: continue

                # T·∫°o ID (d√πng ID trong file ho·∫∑c t·∫°o m·ªõi)
                doc_id = str(item.get("id", uuid.uuid4().hex))
                
                ids.append(doc_id)
                documents.append(content)

                # X·ª≠ l√Ω Metadata (Chroma kh√¥ng nh·∫≠n gi√° tr·ªã None)
                raw_meta = item.get("metadata", {})
                clean_meta = {
                    "source_url": str(item.get("url", "")),
                    "title": str(item.get("title", "")),
                    "hierarchy": str(item.get("hierarchy", "")),
                    "source": str(raw_meta.get("source_domain", "BCA"))
                }
                metadatas.append(clean_meta)

            # 4. N·∫°p v√†o ChromaDB theo l√¥ (Batch)
            batch_size = 100
            for i in range(0, len(ids), batch_size):
                collection.add(
                    ids=ids[i : i+batch_size],
                    documents=documents[i : i+batch_size],
                    metadatas=metadatas[i : i+batch_size]
                )
            
            return collection, f"SUCCESS: ƒê√£ n·∫°p m·ªõi {len(ids)} chunks."
        
        else:
            return collection, f"SUCCESS: D·ªØ li·ªáu ƒë√£ c√≥ s·∫µn ({collection.count()} chunks)."

    except Exception as e:
        return None, f"ERROR: {str(e)}"

# --- G·ªåI H√ÄM N·∫†P D·ªÆ LI·ªÜU V√Ä HI·ªÇN TH·ªä TR·∫†NG TH√ÅI ---
with st.spinner("ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng tri th·ª©c... (L·∫ßn ƒë·∫ßu c√≥ th·ªÉ m·∫•t 1-2 ph√∫t)"):
    collection, status_msg = initialize_database()

if collection is None:
    st.error(f"‚ùå L·ªói kh·ªüi t·∫°o: {status_msg}")
    st.stop()
else:
    # Hi·ªÉn th·ªã th√†nh c√¥ng ·ªü sidebar
    st.sidebar.success(f"üì¶ D·ªØ li·ªáu: **{collection.count()}** chunks")

# ================= 4. LOGIC TR·∫¢ L·ªúI C√ÇU H·ªéI (RAG) =================
def query_ai(question):
    try:
        # 1. T√¨m ki·∫øm d·ªØ li·ªáu li√™n quan
        results = collection.query(
            query_texts=[question],
            n_results=5, # L·∫•y 5 ƒëo·∫°n vƒÉn b·∫£n li√™n quan nh·∫•t
            include=["documents", "metadatas"]
        )

        # 2. T·∫°o ng·ªØ c·∫£nh (Context)
        context_text = ""
        sources = []
        
        if results["documents"]:
            for i, doc in enumerate(results["documents"][0]):
                meta = results["metadatas"][0][i]
                source_title = meta.get('hierarchy', meta.get('title', 'Th√¥ng tin'))
                context_text += f"---\nNgu·ªìn: {source_title}\nN·ªôi dung: {doc}\n"
                
                # L∆∞u link ngu·ªìn ƒë·ªÉ hi·ªÉn th·ªã
                url = meta.get('source_url', '')
                if url: sources.append(url)

        if not context_text:
            return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong c∆° s·ªü d·ªØ li·ªáu.", []

        # 3. G·ª≠i cho Gemini
        prompt = f"""
        B·∫°n l√† tr·ª£ l√Ω ·∫£o h·ªó tr·ª£ ph√°p lu·∫≠t Vi·ªát Nam.
        H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y.
        
        Y√äU C·∫¶U:
        - Tr·∫£ l·ªùi ch√≠nh x√°c, ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu.
        - N·∫øu l√† quy tr√¨nh, h√£y li·ªát k√™ t·ª´ng b∆∞·ªõc.
        - Tuy·ªát ƒë·ªëi kh√¥ng b·ªãa ƒë·∫∑t th√¥ng tin n·∫øu kh√¥ng c√≥ trong ng·ªØ c·∫£nh.
        
        TH√îNG TIN THAM KH·∫¢O:
        {context_text}
        
        C√ÇU H·ªéI: {question}
        """
        
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(prompt)
        return response.text, list(set(sources))

    except Exception as e:
        return f"ƒê√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω: {str(e)}", []

# ================= 5. GIAO DI·ªÜN CHAT =================
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ v·ªÅ th·ªß t·ª•c Th∆∞·ªùng tr√∫, T·∫°m tr√∫, H·ªô chi·∫øu...?"}]

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    # Hi·ªÉn th·ªã c√¢u h·ªèi ng∆∞·ªùi d√πng
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # X·ª≠ l√Ω v√† tr·∫£ l·ªùi
    with st.chat_message("assistant"):
        with st.spinner("ƒêang tra c·ª©u quy ƒë·ªãnh..."):
            ans, source_links = query_ai(prompt)
            
            # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
            st.markdown(ans)
            
            # Hi·ªÉn th·ªã ngu·ªìn tham kh·∫£o (n·∫øu c√≥)
            if source_links:
                st.markdown("**üîó Ngu·ªìn tham kh·∫£o:**")
                for link in source_links:
                    st.markdown(f"- [{link}]({link})")
            
            # L∆∞u v√†o l·ªãch s·ª≠ chat (ch·ªâ l∆∞u text c√¢u tr·∫£ l·ªùi)
            st.session_state.messages.append({"role": "assistant", "content": ans})
