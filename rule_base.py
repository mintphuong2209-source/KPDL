import streamlit as st
import json
import os
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# ================= C·∫§U H√åNH =================
# T√™n file d·ªØ li·ªáu b·∫°n ƒë√£ upload l√™n GitHub (n·∫±m c√πng th∆∞ m·ª•c v·ªõi app.py)
JSON_FILE = "all_procedures_normalized.json" 
COLLECTION_NAME = "dichvucong_rag"

# C·∫•u h√¨nh Page
st.set_page_config(page_title="Chatbot H·ªó Tr·ª£ C∆∞ Tr√∫", layout="centered")
st.title("ü§ñ Chatbot T∆∞ V·∫•n Th·ªß T·ª•c C∆∞ Tr√∫")

# ================= X·ª¨ L√ù API KEY =================
# C√°ch 1: L·∫•y t·ª´ st.secrets (Khuy√™n d√πng khi deploy l√™n Streamlit Cloud)
# C√°ch 2: Nh·∫≠p tr·ª±c ti·∫øp (Ch·ªâ d√πng test nhanh, kh√¥ng b·∫£o m·∫≠t)
api_key = st.secrets.get("GEMINI_API_KEY") 

if not api_key:
    api_key = st.text_input("Nh·∫≠p Google AI Studio API Key:", type="password")
    if not api_key:
        st.info("Vui l√≤ng nh·∫≠p API Key ƒë·ªÉ ti·∫øp t·ª•c.")
        st.stop()

genai.configure(api_key=api_key)

# ================= H√ÄM LOAD D·ªÆ LI·ªÜU & VECTOR DB =================
# D√πng @st.cache_resource ƒë·ªÉ ch·ªâ ch·∫°y 1 l·∫ßn duy nh·∫•t khi kh·ªüi ƒë·ªông app
@st.cache_resource
def initialize_vector_db():
    # 1. Kh·ªüi t·∫°o Embedding Model (D√πng model nh·∫π h∆°n ch√∫t ƒë·ªÉ ch·∫°y m∆∞·ª£t tr√™n Cloud Free)
    # N·∫øu mu·ªën ch√≠nh x√°c cao h∆°n nh∆∞ng ch·∫≠m h∆°n, ƒë·ªïi l·∫°i th√†nh "BAAI/bge-m3"
    EMBEDDING_MODEL = "BAAI/bge-m3" 
    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=EMBEDDING_MODEL
    )
    
    # 2. T·∫°o Client ChromaDB (Ch·∫°y trong memory ƒë·ªÉ tr√°nh l·ªói path tr√™n Cloud)
    chroma_client = chromadb.Client()
    
    try:
        collection = chroma_client.get_or_create_collection(
            name=COLLECTION_NAME,
            embedding_function=embedding_function
        )
        
        # 3. Ki·ªÉm tra xem ƒë√£ c√≥ d·ªØ li·ªáu ch∆∞a, n·∫øu ch∆∞a th√¨ n·∫°p t·ª´ JSON
        if collection.count() == 0:
            if not os.path.exists(JSON_FILE):
                st.error(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu: {JSON_FILE}. Vui l√≤ng upload file n√†y l√™n GitHub.")
                return None
                
            with open(JSON_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ n·∫°p (Batch processing ƒë·ªÉ tr√°nh qu√° t·∫£i RAM)
            ids = []
            documents = []
            metadatas = []
            
            # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng n·∫øu file qu√° l·ªõn (V√≠ d·ª• test v·ªõi 500 d√≤ng ƒë·∫ßu ti√™n)
            # data = data[:500] 
            
            for item in data:
                ids.append(item["id"])
                documents.append(item["content_text"])
                
                # X·ª≠ l√Ω metadata
                meta = item.get("metadata", {}).copy()
                meta.update({
                    "url": item.get("url", ""),
                    "title": item.get("title", ""),
                    "hierarchy": item.get("hierarchy", ""),
                })
                # ƒê·∫£m b·∫£o metadata kh√¥ng ch·ª©a None value (ChromaDB kh√¥ng ch·ªãu None)
                clean_meta = {k: (v if v is not None else "") for k, v in meta.items()}
                metadatas.append(clean_meta)
            
            # N·∫°p v√†o DB
            batch_size = 100
            for i in range(0, len(ids), batch_size):
                collection.add(
                    ids=ids[i:i+batch_size],
                    documents=documents[i:i+batch_size],
                    metadatas=metadatas[i:i+batch_size]
                )
                
        return collection
        
    except Exception as e:
        st.error(f"L·ªói kh·ªüi t·∫°o Vector DB: {str(e)}")
        return None

# G·ªçi h√†m kh·ªüi t·∫°o
with st.spinner("ƒêang kh·ªüi t·∫°o c∆° s·ªü d·ªØ li·ªáu tri th·ª©c (L·∫ßn ƒë·∫ßu s·∫Ω h∆°i l√¢u)..."):
    collection = initialize_vector_db()

if not collection:
    st.stop()

# ================= LOGIC RAG & CHAT =================
def query_rag(query_text, top_k=3):
    # 1. T√¨m ki·∫øm trong Vector DB
    results = collection.query(
        query_texts=[query_text],
        n_results=top_k,
        include=["documents", "metadatas"]
    )
    
    # 2. T·∫°o Context
    context_parts = []
    sources = []
    
    if results["documents"]:
        for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
            hierarchy = meta.get('hierarchy', meta.get('title', 'M·ª•c'))
            url = meta.get('url', '#')
            context_parts.append(f"[{hierarchy}]\n{doc}")
            sources.append(f"- [{hierarchy}]({url})")
            
    context = "\n\n".join(context_parts)
    
    # 3. T·∫°o Prompt
    prompt = f"""
    B·∫°n l√† tr·ª£ l√Ω ·∫£o h·ªó tr·ª£ ph√°p l√Ω v·ªÅ c∆∞ tr√∫ Vi·ªát Nam.
    D·ª±a v√†o th√¥ng tin sau ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. 
    N·∫øu kh√¥ng c√≥ th√¥ng tin trong ng·ªØ c·∫£nh, h√£y n√≥i "T√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin trong vƒÉn b·∫£n ph√°p lu·∫≠t hi·ªán c√≥".
    
    NG·ªÆ C·∫¢NH:
    {context}
    
    C√ÇU H·ªéI: {query_text}
    
    TR·∫¢ L·ªúI (Ng·∫Øn g·ªçn, format ƒë·∫πp):
    """
    
    # 4. G·ªçi Gemini
    model = genai.GenerativeModel('gemini-1.5-flash') # D√πng b·∫£n Flash cho nhanh v√† r·∫ª
    response = model.generate_content(prompt)
    return response.text, sources

# ================= GIAO DI·ªÜN CHAT =================
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ v·ªÅ th·ªß t·ª•c th∆∞·ªùng tr√∫, t·∫°m tr√∫?"}]

# Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# X·ª≠ l√Ω khi ng∆∞·ªùi d√πng nh·∫≠p
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    # Hi·ªán c√¢u h·ªèi ng∆∞·ªùi d√πng
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # X·ª≠ l√Ω tr·∫£ l·ªùi
    with st.chat_message("assistant"):
        with st.spinner("ƒêang tra c·ª©u lu·∫≠t..."):
            try:
                answer, sources = query_rag(prompt)
                
                # Format c√¢u tr·∫£ l·ªùi k√®m ngu·ªìn
                full_response = f"{answer}\n\n**Ngu·ªìn tham kh·∫£o:**\n" + "\n".join(set(sources))
                
                st.markdown(full_response)
                st.session_state.messages.append({"role": "assistant", "content": full_response})
                
            except Exception as e:
                st.error(f"C√≥ l·ªói x·∫£y ra: {str(e)}")
