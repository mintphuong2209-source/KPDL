# ================= FIX L·ªñI SQLITE TR√äN STREAMLIT CLOUD =================
# B·∫ÆT BU·ªòC: Ph·∫£i ƒë·ªÉ 3 d√≤ng n√†y ·ªü tr√™n c√πng file
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# =======================================================================

import streamlit as st
import json
import os
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# ================= C·∫§U H√åNH =================
JSON_FILE = "all_procedures_normalized.json" 
COLLECTION_NAME = "dichvucong_rag"

st.set_page_config(page_title="Chatbot H·ªó Tr·ª£ C∆∞ Tr√∫", layout="wide")
st.title("ü§ñ Chatbot T∆∞ V·∫•n Th·ªß T·ª•c C∆∞ Tr√∫")

# ================= 1. X·ª¨ L√ù API KEY & CH·ªåN MODEL (AUTO) =================
st.sidebar.header("‚öôÔ∏è C·∫•u h√¨nh")

# L·∫•y Key t·ª´ Secrets ho·∫∑c nh·∫≠p tay
api_key = st.secrets.get("GEMINI_API_KEY") 
if not api_key:
    api_key = st.sidebar.text_input("Nh·∫≠p Google AI Studio API Key:", type="password")
    if not api_key:
        st.warning("üëâ Vui l√≤ng nh·∫≠p API Key ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
        st.stop()

genai.configure(api_key=api_key)

# T·ª± ƒë·ªông t√¨m model Gemini kh·∫£ d·ª•ng ƒë·ªÉ tr√°nh l·ªói 404
try:
    available_models = []
    for m in genai.list_models():
        if 'generateContent' in m.supported_generation_methods:
            # ∆Øu ti√™n c√°c model Pro ho·∫∑c Flash
            if 'gemini' in m.name:
                available_models.append(m.name)
    
    if available_models:
        # Cho ph√©p ng∆∞·ªùi d√πng ch·ªçn model n·∫øu t√¨m th·∫•y nhi·ªÅu
        SELECTED_MODEL = st.sidebar.selectbox("Ch·ªçn Model AI:", available_models, index=0)
    else:
        st.sidebar.error("‚ùå Kh√¥ng t√¨m th·∫•y model Gemini n√†o cho Key n√†y.")
        st.stop()
        
except Exception as e:
    st.sidebar.error(f"L·ªói k·∫øt n·ªëi API: {e}")
    st.stop()

# ================= 2. H√ÄM LOAD D·ªÆ LI·ªÜU T·ªêI ∆ØU RAM =================
@st.cache_resource
def initialize_vector_db():
    # üëâ D√ôNG MODEL NH·∫∏ NH·∫§T ƒê·ªÇ KH√îNG B·ªä OUT OF MEMORY
    EMBEDDING_MODEL = "keepitreal/vietnamese-sbert"
    
    try:
        # Load embedding model
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=EMBEDDING_MODEL
        )
        
        # T·∫°o Client ChromaDB (Ch·∫°y tr√™n RAM - Ephemeral)
        chroma_client = chromadb.Client()
        collection = chroma_client.get_or_create_collection(
            name=COLLECTION_NAME,
            embedding_function=embedding_function
        )
        
        # N·∫øu ch∆∞a c√≥ d·ªØ li·ªáu th√¨ n·∫°p m·ªõi
        if collection.count() == 0:
            if not os.path.exists(JSON_FILE):
                st.error(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file: {JSON_FILE}. H√£y upload file n√†y l√™n GitHub.")
                return None
                
            with st.spinner("ƒêang n·∫°p d·ªØ li·ªáu (Ch·∫ø ƒë·ªô ti·∫øt ki·ªám RAM)..."):
                with open(JSON_FILE, "r", encoding="utf-8") as f:
                    data = json.load(f)
                
                # T√°ch d·ªØ li·ªáu ra c√°c list
                ids = [item["id"] for item in data]
                documents = [item["content_text"] for item in data]
                metadatas = []
                
                for item in data:
                    meta = item.get("metadata", {}).copy()
                    meta.update({
                        "url": item.get("url", ""),
                        "title": item.get("title", ""),
                        "hierarchy": item.get("hierarchy", ""),
                    })
                    # X√≥a gi√° tr·ªã None (ChromaDB kh√¥ng ch·ªãu None)
                    clean_meta = {k: (v if v is not None else "") for k, v in meta.items()}
                    metadatas.append(clean_meta)
                
                # üëâ N·∫°p Batch nh·ªè (40) ƒë·ªÉ tr√°nh tr√†n RAM
                batch_size = 40
                progress_bar = st.sidebar.progress(0)
                
                for i in range(0, len(ids), batch_size):
                    collection.add(
                        ids=ids[i:i+batch_size],
                        documents=documents[i:i+batch_size],
                        metadatas=metadatas[i:i+batch_size]
                    )
                    # C·∫≠p nh·∫≠t thanh ti·∫øn tr√¨nh
                    progress = min((i + batch_size) / len(ids), 1.0)
                    progress_bar.progress(progress)
                
                progress_bar.empty()
                st.toast(f"ƒê√£ n·∫°p xong {len(ids)} chunks!", icon="‚úÖ")
                
        return collection
        
    except Exception as e:
        st.error(f"L·ªói kh·ªüi t·∫°o DB: {str(e)}")
        return None

# G·ªçi h√†m kh·ªüi t·∫°o
collection = initialize_vector_db()

if not collection:
    st.stop()
    
# HI·ªÜN S·ªê L∆Ø·ª¢NG CHUNK L√äN SIDEBAR
st.sidebar.success(f"üì¶ D·ªØ li·ªáu ƒë√£ n·∫°p: **{collection.count()}** chunks")

# ================= 3. LOGIC H·ªéI ƒê√ÅP (RAG) =================
def query_rag(query_text, model_name, top_k=12): # üëâ TƒÉng top_k l√™n 12 ƒë·ªÉ l·∫•y nhi·ªÅu th√¥ng tin h∆°n
    try:
        # T√¨m ki·∫øm vector
        results = collection.query(
            query_texts=[query_text],
            n_results=top_k,
            include=["documents", "metadatas"]
        )
        
        context_parts = []
        sources = []
        
        if results["documents"]:
            for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
                hierarchy = meta.get('hierarchy', meta.get('title', 'Th√¥ng tin'))
                url = meta.get('url', '#')
                
                # T·∫°o context
                context_parts.append(f"--- N·ªòI DUNG T·ª™ M·ª§C: {hierarchy} ---\n{doc}")
                
                # L∆∞u ngu·ªìn
                sources.append(f"- [{hierarchy}]({url})")
                
        context = "\n\n".join(context_parts)
        
        # Prompt chi ti·∫øt
        prompt = f"""
        B·∫°n l√† tr·ª£ l√Ω ·∫£o h·ªó tr·ª£ ph√°p l√Ω v·ªÅ th·ªß t·ª•c h√†nh ch√≠nh c∆∞ tr√∫ t·∫°i Vi·ªát Nam.
        Nhi·ªám v·ª•: Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng CH·ªà d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y.
        
        Y√äU C·∫¶U:
        1. Tr·∫£ l·ªùi chi ti·∫øt, t·ª´ng b∆∞·ªõc n·∫øu l√† quy tr√¨nh.
        2. N·∫øu th√¥ng tin c√≥ trong ng·ªØ c·∫£nh, h√£y tr√≠ch d·∫´n.
        3. N·∫øu KH√îNG t√¨m th·∫•y th√¥ng tin trong ng·ªØ c·∫£nh, h√£y n√≥i: "Xin l·ªói, t√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ trong d·ªØ li·ªáu hi·ªán c√≥."
        4. Kh√¥ng t·ª± b·ªãa ra th√¥ng tin ph√°p lu·∫≠t.

        NG·ªÆ C·∫¢NH TH√îNG TIN (D·ªØ li·ªáu t√¨m ƒë∆∞·ª£c):
        {context}
        
        C√ÇU H·ªéI C·ª¶A NG∆Ø·ªúI D√ôNG: {query_text}
        
        TR·∫¢ L·ªúI:
        """
        
        # G·ªçi Gemini v·ªõi model ƒë√£ ch·ªçn t·ª± ƒë·ªông
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(prompt)
        return response.text, sources, context
        
    except Exception as e:
        return f"L·ªói x·ª≠ l√Ω: {str(e)}", [], ""

# ================= 4. GIAO DI·ªÜN CHAT =================
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Xin ch√†o! B·∫°n c·∫ßn t√¨m hi·ªÉu th·ªß t·ª•c g√¨ (v√≠ d·ª•: ƒêƒÉng k√Ω th∆∞·ªùng tr√∫, T√°ch h·ªô...)? "}]

# Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# X·ª≠ l√Ω input
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    # User message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Bot response
    with st.chat_message("assistant"):
        with st.spinner("ƒêang tra c·ª©u d·ªØ li·ªáu ph√°p lu·∫≠t..."):
            answer, sources, debug_context = query_rag(prompt, SELECTED_MODEL)
            
            # X·ª≠ l√Ω ngu·ªìn tham kh·∫£o (x√≥a tr√πng l·∫∑p)
            unique_sources = list(set(sources))
            
            if unique_sources:
                full_response = f"{answer}\n\n---\n**üìö Ngu·ªìn tham kh·∫£o:**\n" + "\n".join(unique_sources)
            else:
                full_response = answer
            
            st.markdown(full_response)
            
            # Debug: Cho ph√©p xem nh·ªØng g√¨ AI ƒë√£ ƒë·ªçc ƒë∆∞·ª£c (ƒë·ªÉ ki·ªÉm tra xem n√≥ c√≥ ƒë·ªçc ƒë√∫ng chunk kh√¥ng)
            with st.expander("üïµÔ∏è [Debug] Xem d·ªØ li·ªáu AI t√¨m th·∫•y"):
                st.text(debug_context)
            
            st.session_state.messages.append({"role": "assistant", "content": full_response})
