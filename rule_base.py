# -*- coding: utf-8 -*-
"""Rule base.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cq9JilvrM6ZLqxpqfleFy9xfEgO1CDBf
"""

!pip install -q requests
!pip install -q bs4
!pip install -q html2text
!pip install -q chromadb
!pip install -q google-generativeai

from google.colab import drive
drive.mount('/content/drive')

!pip install streamlit
!pip install chromadb
!pip install google-generativeai
!pip install torch torchvision torchaudio
!pip install sentence-transformers
!pip install beautifulsoup4 requests # cho crawl n·∫øu c·∫ßn t√≠ch h·ª£p

import uuid
from datetime import date
import hashlib
import re
import json
import html2text

import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urlparse

import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# ================== DANH S√ÅCH URL TH∆Ø·ªúNG TR√ö (DVC)==================
THUONG_TRU_URLS = [
    {"procedure_code": "1.004222", "url": "https://dichvucong.gov.vn/p/home/dvc-chi-tiet-thu-tuc-nganh-doc.html?ma_thu_tuc=1.004222"}
]

# ================== DANH S√ÅCH URL T·∫†M TR√ö (DVC) ==================
TAM_TRU_URLS = [
    {"procedure_code": "1.004194", "url": "https://dichvucong.gov.vn/p/home/dvc-chi-tiet-thu-tuc-nganh-doc.html?ma_thu_tuc=1.004194"},###
    {"procedure_code": "1.002755", "url": "https://dichvucong.gov.vn/p/home/dvc-chi-tiet-thu-tuc-nganh-doc.html?ma_thu_tuc=1.002755"},###
    {"procedure_code": "1.010028", "url": "https://dichvucong.gov.vn/p/home/dvc-chi-tiet-thu-tuc-nganh-doc.html?ma_thu_tuc=1.010028"},###
]

# ================== DANH S√ÅCH URL TH∆Ø·ªúNG TR√ö (BCA) ==================
THUONG_TRU_BCA_URLS = [
    {"procedure_code": "26360", "url": "https://dichvucong.bocongan.gov.vn/bocongan/bothutuc/tthc?matt=26360"},###
    {"procedure_code": "26348", "url": "https://dichvucong.bocongan.gov.vn/bocongan/bothutuc/tthc?matt=26348"},###
]

# ================== DANH S√ÅCH URL T·∫†M TR√ö (BCA) ==================
TAM_TRU_BCA_URLS = [
    {"procedure_code": "26356", "url": "https://dichvucong.bocongan.gov.vn/bocongan/bothutuc/tthc?matt=26356"},###
    {"procedure_code": "26552", "url": "https://dichvucong.bocongan.gov.vn/bocongan/bothutuc/tthc?matt=26552"},###
    {"procedure_code": "26345", "url": "https://dichvucong.bocongan.gov.vn/bocongan/bothutuc/tthc?matt=26345"},###
]

# ================== DANH S√ÅCH URL KHAI B√ÅO ==================
KHAI_BAO_THONG_TIN_URLS = [
    {"procedure_code": "26492", "url": "https://dichvucong.bocongan.gov.vn/bocongan/bothutuc/tthc?matt=26492"}
]

# ================== DANH S√ÅCH URL ƒêI·ªÄU CH·ªàNH ==================
DIEU_CHINH_THONG_TIN_CU_TRU_URLS = [
    {"procedure_code": "26498", "url": "https://dichvucong.bocongan.gov.vn/bocongan/bothutuc/tthc?matt=26498"}
]

# ================== DANH S√ÅCH URL KHAI B√ÅO T·∫†M V·∫ÆNG ==================
KHAI_BAO_TAM_VANG_URLS = [
    {"procedure_code": "26351", "url": "https://dichvucong.bocongan.gov.vn/bocongan/bothutuc/tthc?matt=26351"}
]

# ================== DANH S√ÅCH URL T√ÅCH H·ªò ==================
TACH_HO_URLS = [
    {"procedure_code": "26499", "url": "https://dichvucong.bocongan.gov.vn/bocongan/bothutuc/tthc?matt=26499"}
]

# ================== DANH S√ÅCH URL TH√îNG B√ÅO L∆ØU TR√ö ==================
LUU_TRU_URLS = [
    {"procedure_code": "26346", "url": "https://dichvucong.bocongan.gov.vn/bocongan/bothutuc/tthc?matt=26346"}
]

def clean_text(text):
    return re.sub(r'\s+', ' ', text).strip() if text else ""

def make_chunk(metadata, content, chunk_type, hierarchy, url, title, rows=0, cols=0):
    key = f"{url}{hierarchy}{content[:200]}"
    hash_id = hashlib.sha256(key.encode()).hexdigest()[:20]

    chunk = {
        "id": f"{hash_id}-{uuid.uuid4().hex[:8]}",
        "url": url,
        "title": title,
        "chunk_type": chunk_type,
        "hierarchy": hierarchy,
        "content_text": content.strip(),
        "metadata": {
            "source_domain": urlparse(url).netloc,
            "tokens": len(content.split()),
            "has_table": chunk_type == "table",
            "crawl_date": date.today().strftime("%Y-%m-%d"),
            "procedure_code": metadata.get("procedure_code", ""),
            "category": metadata.get("category", ""),
            "offical_annouce": "2196/Qƒê-UBND ng√†y 19/6/2024",
            "is_entire": metadata.get("is_entire", False)
        }
    }
    if chunk_type == "table":
        chunk["metadata"].update({"row_count": rows, "col_count": cols})
    return chunk

def count_table_cells(table):
    rows = table.find_all("tr")
    cols = 0
    if rows:
        cols = len(rows[0].find_all(["th", "td"]))
    return len(rows), cols

def crawl_and_chunk(metadata, url, headers):
    resp = requests.get(url, headers=headers, timeout=20)
    resp.encoding = "utf-8"
    soup = BeautifulSoup(resp.text, "html.parser")

    title = clean_text(soup.h1.get_text()) if soup.h1 else "Kh√¥ng c√≥ ti√™u ƒë·ªÅ"
    chunks = []
    hierarchy_stack = []
    h = html2text.HTML2Text()

    # V√πng n·ªôi dung ch√≠nh c·ªßa dichvucong
    main = soup.find("div", class_="row")
    if main:
        main = main.find("div", class_="col-sm-8")
    if not main:
        main = soup.body

    def should_skip(elem):
        if not elem.get("class"):
            return False
        classes = " ".join(elem["class"])
        skip_words = ["breadcrumb", "header", "footer", "navbar", "sidebar", "related", "comment"]
        return any(word in classes for word in skip_words)

    def process(elem):
        nonlocal hierarchy_stack

        if should_skip(elem):
            return False

        has_processed_child = False

        # Duy·ªát con tr∆∞·ªõc
        for child in elem.children:
            if getattr(child, "name", None):
                if process(child):
                    has_processed_child = True

        # X·ª≠ l√Ω ch√≠nh element n√†y
        # 1. Table
        if elem.name == "table" and not elem.get("data-processed"):
            md = h.handle(str(elem)).strip()
            if len(md) > 50:
                h_text = " > ".join(hierarchy_stack) or title
                rows, cols = count_table_cells(elem)
                chunks.append(make_chunk(metadata, md, "table", h_text, url, title, rows, cols))
                elem.attrs["data-processed"] = "true"
            return True

        # 2. List
        if elem.name in ["ul", "ol"]:
            items = [clean_text(li.get_text()) for li in elem.find_all("li", recursive=False)]
            if items:
                content = "\n- " + "\n- ".join(items)
                h_text = " > ".join(hierarchy_stack) or title
                chunks.append(make_chunk(metadata, content, "list", h_text, url, title))
            return True

        # 3. Heading
        if elem.name in ["h1", "h2", "h3", "h4", "h5", "h6"]:
            level = int(elem.name[1])
            text = clean_text(elem.get_text())
            if len(hierarchy_stack) >= level:
                hierarchy_stack[level-1] = text
                hierarchy_stack = hierarchy_stack[:level]
            else:
                hierarchy_stack.append(text)

            h_text = " > ".join(hierarchy_stack)
            chunks.append(make_chunk(metadata, text, "header", h_text, url, title))
            return True

        # 4. ƒêo·∫°n vƒÉn / div ch·ªâ l·∫•y n·∫øu kh√¥ng c√≥ con ƒë·∫∑c bi·ªát
        if elem.name in ["p", "div", "section", "article"]:
            if has_processed_child or elem.get("data-processed") or elem.find(attrs={"data-processed": "true"}):
                return True
            text = clean_text(elem.get_text())
            if 15 < len(text) < 3000:
                h_text = " > ".join(hierarchy_stack) or title
                chunks.append(make_chunk(metadata, text, "paragraph", h_text, url, title))
            return True

        return has_processed_child

    # B·∫Øt ƒë·∫ßu duy·ªát
    for el in main.descendants:
        if getattr(el, "name", None):
            process(el)

    return chunks

import json
from urllib.parse import urlparse
from datetime import date

# ================== C·∫§U H√åNH ==================
DATA_FOLDER = "/content/drive/MyDrive/RAG"
OUTPUT_FILE = f"{DATA_FOLDER}/thuong_tru.json"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

# ================== CRAWL + CHUNK ==================
all_chunks = []

for idx, item in enumerate(THUONG_TRU_URLS, start=1):
    url = item["url"]
    procedure_code = item["procedure_code"]

    print(f"üîπ ({idx}/{len(THUONG_TRU_URLS)}) ƒê√£ crawl: {procedure_code}")

    metadata = {
        "procedure_code": procedure_code,
        "category": "Th·ªß t·ª•c ƒëƒÉng k√Ω th∆∞·ªùng tr√∫",
        "official_announce": "68/2020/QH14 ng√†y 13/11/2020",
        "is_entire": True
    }

    try:
        chunks = crawl_and_chunk(metadata, url, HEADERS)
        all_chunks.extend(chunks)
        print(f"   ‚úÖ Thu ƒë∆∞·ª£c {len(chunks)} chunks")
    except Exception as e:
        print(f"   ‚ùå L·ªói crawl {procedure_code}: {e}")

# ================== L∆ØU FILE CHUNG ==================
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(all_chunks, f, ensure_ascii=False, indent=2)

print("\nüéâ HO√ÄN T·∫§T!")
print(f"üìÑ File: {OUTPUT_FILE}")
print(f"üì¶ T·ªïng s·ªë chunk: {len(all_chunks)}")

import json
from urllib.parse import urlparse
from datetime import date

# ================== C·∫§U H√åNH ==================
DATA_FOLDER = "/content/drive/MyDrive/RAG"
OUTPUT_FILE = f"{DATA_FOLDER}/tam_tru.json"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

# ================== CRAWL + CHUNK ==================
all_chunks = []

for idx, item in enumerate(TAM_TRU_URLS, start=1):
    url = item["url"]
    procedure_code = item["procedure_code"]

    print(f"üîπ ({idx}/{len(TAM_TRU_URLS)}) ƒê√£ crawl: {procedure_code}")

    metadata = {
        "procedure_code": procedure_code,
        "category": "Th·ªß t·ª•c ƒëƒÉng k√Ω,gia h·∫°n, x√≥a ƒëƒÉng k√Ω t·∫°m tr√∫",
        "official_announce": "68/2020/QH14 ng√†y 13/11/2020",
        "is_entire": True
    }

    try:
        chunks = crawl_and_chunk(metadata, url, HEADERS)
        all_chunks.extend(chunks)
        print(f"   ‚úÖ Thu ƒë∆∞·ª£c {len(chunks)} chunks")
    except Exception as e:
        print(f"   ‚ùå L·ªói crawl {procedure_code}: {e}")

# ================== L∆ØU FILE CHUNG ==================
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(all_chunks, f, ensure_ascii=False, indent=2)

print("\nüéâ HO√ÄN T·∫§T!")
print(f"üìÑ File: {OUTPUT_FILE}")
print(f"üì¶ T·ªïng s·ªë chunk: {len(all_chunks)}")

import json
from urllib.parse import urlparse
from datetime import date

# ================== C·∫§U H√åNH ==================
DATA_FOLDER = "/content/drive/MyDrive/RAG"
OUTPUT_FILE = f"{DATA_FOLDER}/thuong_tru_bca.json"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

# ================== CRAWL + CHUNK ==================
all_chunks = []

for idx, item in enumerate(THUONG_TRU_BCA_URLS, start=1):
    url = item["url"]
    procedure_code = item["procedure_code"]

    print(f"üîπ ({idx}/{len(THUONG_TRU_BCA_URLS)}) ƒê√£ crawl: {procedure_code}")

    metadata = {
        "procedure_code": procedure_code,
        "category": "Th·ªß t·ª•c ƒëƒÉng k√Ω th∆∞·ªùng tr√∫",
        "official_announce": "68/2020/QH14 ng√†y 13/11/2020",
        "is_entire": True
    }

    try:
        chunks = crawl_and_chunk(metadata, url, HEADERS)
        all_chunks.extend(chunks)
        print(f"   ‚úÖ Thu ƒë∆∞·ª£c {len(chunks)} chunks")
    except Exception as e:
        print(f"   ‚ùå L·ªói crawl {procedure_code}: {e}")

# ================== L∆ØU FILE CHUNG ==================
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(all_chunks, f, ensure_ascii=False, indent=2)

print("\nüéâ HO√ÄN T·∫§T!")
print(f"üìÑ File: {OUTPUT_FILE}")
print(f"üì¶ T·ªïng s·ªë chunk: {len(all_chunks)}")

import json
from urllib.parse import urlparse
from datetime import date

# ================== C·∫§U H√åNH ==================
DATA_FOLDER = "/content/drive/MyDrive/RAG"
OUTPUT_FILE = f"{DATA_FOLDER}/tam_tru_bca.json"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

# ================== CRAWL + CHUNK ==================
all_chunks = []

for idx, item in enumerate(TAM_TRU_BCA_URLS, start=1):
    url = item["url"]
    procedure_code = item["procedure_code"]

    print(f"üîπ ({idx}/{len(TAM_TRU_BCA_URLS)}) ƒê√£ crawl: {procedure_code}")

    metadata = {
        "procedure_code": procedure_code,
        "category": "Th·ªß t·ª•c ƒëƒÉng k√Ω,gia h·∫°n, x√≥a ƒëƒÉng k√Ω t·∫°m tr√∫",
        "official_announce": "68/2020/QH14 ng√†y 13/11/2020",
        "is_entire": True
    }

    try:
        chunks = crawl_and_chunk(metadata, url, HEADERS)
        all_chunks.extend(chunks)
        print(f"   ‚úÖ Thu ƒë∆∞·ª£c {len(chunks)} chunks")
    except Exception as e:
        print(f"   ‚ùå L·ªói crawl {procedure_code}: {e}")

# ================== L∆ØU FILE CHUNG ==================
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(all_chunks, f, ensure_ascii=False, indent=2)

print("\nüéâ HO√ÄN T·∫§T!")
print(f"üìÑ File: {OUTPUT_FILE}")
print(f"üì¶ T·ªïng s·ªë chunk: {len(all_chunks)}")

import json
from urllib.parse import urlparse
from datetime import date

# ================== C·∫§U H√åNH ==================
DATA_FOLDER = "/content/drive/MyDrive/RAG"
OUTPUT_FILE = f"{DATA_FOLDER}/khai_bao_thong_tin.json"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

# ================== CRAWL + CHUNK ==================
all_chunks = []

for idx, item in enumerate(KHAI_BAO_THONG_TIN_URLS, start=1):
    url = item["url"]
    procedure_code = item["procedure_code"]

    print(f"üîπ ({idx}/{len(KHAI_BAO_THONG_TIN_URLS)}) ƒê√£ crawl: {procedure_code}")

    metadata = {
        "procedure_code": procedure_code,
        "category": "Th·ªß t·ª•c khai b√°o th√¥ng tin v·ªÅ c∆∞ tr√∫ ƒë·ªëi v·ªõi ng∆∞·ªùi ch∆∞a ƒë·ªß ƒëi·ªÅu ki·ªán ƒëƒÉng k√Ω th∆∞·ªùng tr√∫, ƒëƒÉng k√Ω t·∫°m tr√∫",
        "official_announce": "68/2020/QH14 ng√†y 13/11/2020",
        "is_entire": True
    }

    try:
        chunks = crawl_and_chunk(metadata, url, HEADERS)
        all_chunks.extend(chunks)
        print(f"   ‚úÖ Thu ƒë∆∞·ª£c {len(chunks)} chunks")
    except Exception as e:
        print(f"   ‚ùå L·ªói crawl {procedure_code}: {e}")

# ================== L∆ØU FILE CHUNG ==================
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(all_chunks, f, ensure_ascii=False, indent=2)

print("\nüéâ HO√ÄN T·∫§T!")
print(f"üìÑ File: {OUTPUT_FILE}")
print(f"üì¶ T·ªïng s·ªë chunk: {len(all_chunks)}")

import json
from urllib.parse import urlparse
from datetime import date

# ================== C·∫§U H√åNH ==================
DATA_FOLDER = "/content/drive/MyDrive/RAG"
OUTPUT_FILE = f"{DATA_FOLDER}/dieu_chinh_thong_tin_cu_tru.json"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

# ================== CRAWL + CHUNK ==================
all_chunks = []

for idx, item in enumerate(DIEU_CHINH_THONG_TIN_CU_TRU_URLS, start=1):
    url = item["url"]
    procedure_code = item["procedure_code"]

    print(f"üîπ ({idx}/{len(DIEU_CHINH_THONG_TIN_CU_TRU_URLS)}) ƒê√£ crawl: {procedure_code}")

    metadata = {
        "procedure_code": procedure_code,
        "category": "Th·ªß t·ª•c ƒëi·ªÅu ch·ªânh th√¥ng tin v·ªÅ c∆∞ tr√∫ trong C∆° s·ªü d·ªØ li·ªáu v·ªÅ c∆∞ tr√∫",
        "official_announce": "68/2020/QH14 ng√†y 13/11/2020",
        "is_entire": True
    }

    try:
        chunks = crawl_and_chunk(metadata, url, HEADERS)
        all_chunks.extend(chunks)
        print(f"   ‚úÖ Thu ƒë∆∞·ª£c {len(chunks)} chunks")
    except Exception as e:
        print(f"   ‚ùå L·ªói crawl {procedure_code}: {e}")

# ================== L∆ØU FILE CHUNG ==================
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(all_chunks, f, ensure_ascii=False, indent=2)

print("\nüéâ HO√ÄN T·∫§T!")
print(f"üìÑ File: {OUTPUT_FILE}")
print(f"üì¶ T·ªïng s·ªë chunk: {len(all_chunks)}")

import json
from urllib.parse import urlparse
from datetime import date

# ================== C·∫§U H√åNH ==================
DATA_FOLDER = "/content/drive/MyDrive/RAG"
OUTPUT_FILE = f"{DATA_FOLDER}/tam_vang.json"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

# ================== CRAWL + CHUNK ==================
all_chunks = []

for idx, item in enumerate(KHAI_BAO_TAM_VANG_URLS, start=1):
    url = item["url"]
    procedure_code = item["procedure_code"]

    print(f"üîπ ({idx}/{len(KHAI_BAO_TAM_VANG_URLS)}) ƒê√£ crawl: {procedure_code}")

    metadata = {
        "procedure_code": procedure_code,
        "category": "Th·ªß t·ª•c khai b√°o t·∫°m v·∫Øng",
        "official_announce": "68/2020/QH14 ng√†y 13/11/2020",
        "is_entire": True
    }

    try:
        chunks = crawl_and_chunk(metadata, url, HEADERS)
        all_chunks.extend(chunks)
        print(f"   ‚úÖ Thu ƒë∆∞·ª£c {len(chunks)} chunks")
    except Exception as e:
        print(f"   ‚ùå L·ªói crawl {procedure_code}: {e}")

# ================== L∆ØU FILE CHUNG ==================
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(all_chunks, f, ensure_ascii=False, indent=2)

print("\nüéâ HO√ÄN T·∫§T!")
print(f"üìÑ File: {OUTPUT_FILE}")
print(f"üì¶ T·ªïng s·ªë chunk: {len(all_chunks)}")

import json
from urllib.parse import urlparse
from datetime import date

# ================== C·∫§U H√åNH ==================
DATA_FOLDER = "/content/drive/MyDrive/RAG"
OUTPUT_FILE = f"{DATA_FOLDER}/tach_ho.json"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

# ================== CRAWL + CHUNK ==================
all_chunks = []

for idx, item in enumerate(TACH_HO_URLS, start=1):
    url = item["url"]
    procedure_code = item["procedure_code"]

    print(f"üîπ ({idx}/{len(TACH_HO_URLS)}) ƒê√£ crawl: {procedure_code}")

    metadata = {
        "procedure_code": procedure_code,
        "category": "Th·ªß t·ª•c t√°ch h·ªô",
        "official_announce": "68/2020/QH14 ng√†y 13/11/2020",
        "is_entire": True
    }

    try:
        chunks = crawl_and_chunk(metadata, url, HEADERS)
        all_chunks.extend(chunks)
        print(f"   ‚úÖ Thu ƒë∆∞·ª£c {len(chunks)} chunks")
    except Exception as e:
        print(f"   ‚ùå L·ªói crawl {procedure_code}: {e}")

# ================== L∆ØU FILE CHUNG ==================
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(all_chunks, f, ensure_ascii=False, indent=2)

print("\nüéâ HO√ÄN T·∫§T!")
print(f"üìÑ File: {OUTPUT_FILE}")
print(f"üì¶ T·ªïng s·ªë chunk: {len(all_chunks)}")

import json
from urllib.parse import urlparse
from datetime import date

# ================== C·∫§U H√åNH ==================
DATA_FOLDER = "/content/drive/MyDrive/RAG"
OUTPUT_FILE = f"{DATA_FOLDER}/luu_tru.json"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

# ================== CRAWL + CHUNK ==================
all_chunks = []

for idx, item in enumerate(LUU_TRU_URLS, start=1):
    url = item["url"]
    procedure_code = item["procedure_code"]

    print(f"üîπ ({idx}/{len(LUU_TRU_URLS)}) ƒê√£ crawl: {procedure_code}")

    metadata = {
        "procedure_code": procedure_code,
        "category": "Th·ªß t·ª•c th√¥ng b√°o l∆∞u tr√∫",
        "official_announce": "68/2020/QH14 ng√†y 13/11/2020",
        "is_entire": True
    }

    try:
        chunks = crawl_and_chunk(metadata, url, HEADERS)
        all_chunks.extend(chunks)
        print(f"   ‚úÖ Thu ƒë∆∞·ª£c {len(chunks)} chunks")
    except Exception as e:
        print(f"   ‚ùå L·ªói crawl {procedure_code}: {e}")

# ================== L∆ØU FILE CHUNG ==================
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(all_chunks, f, ensure_ascii=False, indent=2)

print("\nüéâ HO√ÄN T·∫§T!")
print(f"üìÑ File: {OUTPUT_FILE}")
print(f"üì¶ T·ªïng s·ªë chunk: {len(all_chunks)}")

import json
import re
import unicodedata
import hashlib

# =========================
# C·∫§U H√åNH FILE
# =========================
FILES = [
    (f"{DATA_FOLDER}/thuong_tru.json", f"{DATA_FOLDER}/thuong_tru_normalized.json"),
    (f"{DATA_FOLDER}/tam_tru.json", f"{DATA_FOLDER}/tam_tru_normalized.json"),
    (f"{DATA_FOLDER}/thuong_tru_bca.json", f"{DATA_FOLDER}/thuong_tru_bca_normalized.json"),
    (f"{DATA_FOLDER}/tam_tru_bca.json", f"{DATA_FOLDER}/tam_tru_bca_normalized.json"),
    (f"{DATA_FOLDER}/khai_bao_thong_tin.json", f"{DATA_FOLDER}/khai_bao_thong_tin_normalized.json"),
    (f"{DATA_FOLDER}/dieu_chinh_thong_tin_cu_tru.json", f"{DATA_FOLDER}/dieu_chinh_thong_tin_cu_tru_normalized.json"),
    (f"{DATA_FOLDER}/tam_vang.json", f"{DATA_FOLDER}/tam_vang_normalized.json"),
    (f"{DATA_FOLDER}/tach_ho.json", f"{DATA_FOLDER}/tach_ho_normalized.json"),
    (f"{DATA_FOLDER}/luu_tru.json", f"{DATA_FOLDER}/luu_tru_normalized.json"),
]

# =========================
# H√ÄM CHU·∫®N H√ìA TEXT
# =========================
def normalize_text(text):
    if not text:
        return ""
    text = unicodedata.normalize("NFC", text)  # chu·∫©n unicode ti·∫øng Vi·ªát
    text = re.sub(r"\s+", " ", text)            # b·ªè kho·∫£ng tr·∫Øng d∆∞
    return text.strip()

# =========================
# H√ÄM T·∫†O KEY LO·∫†I TR√ôNG
# =========================
def make_dedup_key(chunk):
    base = (
        chunk.get("url", "") +
        chunk.get("hierarchy", "") +
        chunk.get("content_text", "")
    )
    return hashlib.md5(base.encode("utf-8")).hexdigest()

# =========================
# H√ÄM L√ÄM S·∫†CH + CHU·∫®N H√ìA FILE
# =========================
def clean_and_normalize_file(input_file, output_file):
    with open(input_file, encoding="utf-8") as f:
        chunks = json.load(f)

    cleaned_chunks = []
    seen = set()

    for chunk in chunks:
        content = normalize_text(chunk.get("content_text", ""))

        # ‚ùå b·ªè chunk r√°c / qu√° ng·∫Øn
        if len(content) < 1:
            continue

        hierarchy = normalize_text(chunk.get("hierarchy", ""))
        if not hierarchy:
            hierarchy = normalize_text(chunk.get("title", "Kh√¥ng r√µ m·ª•c"))

        chunk["content_text"] = content
        chunk["hierarchy"] = hierarchy

        # chu·∫©n h√≥a metadata
        if "metadata" in chunk:
            for k, v in chunk["metadata"].items():
                if isinstance(v, str):
                    chunk["metadata"][k] = normalize_text(v)

        # ‚ùå lo·∫°i tr√πng
        dedup_key = make_dedup_key(chunk)
        if dedup_key in seen:
            continue

        seen.add(dedup_key)
        cleaned_chunks.append(chunk)

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(cleaned_chunks, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ {input_file}")
    print(f"   Tr∆∞·ªõc: {len(chunks)} chunks")
    print(f"   Sau  : {len(cleaned_chunks)} chunks\n")

# =========================
# CH·∫†Y CHO 9 FILE
# =========================
for raw_file, normalized_file in FILES:
    clean_and_normalize_file(raw_file, normalized_file)

print("·æø HO√ÄN T·∫§T CHU·∫®N H√ìA 9 FILE")

FILES = [
    f"{DATA_FOLDER}/thuong_tru_normalized.json",
    f"{DATA_FOLDER}/tam_tru_normalized.json",
    f"{DATA_FOLDER}/thuong_tru_bca_normalized.json",
    f"{DATA_FOLDER}/tam_tru_bca_normalized.json",
    f"{DATA_FOLDER}/khai_bao_thong_tin_normalized.json",
    f"{DATA_FOLDER}/dieu_chinh_thong_tin_cu_tru_normalized.json",
    f"{DATA_FOLDER}/tam_vang_normalized.json",
    f"{DATA_FOLDER}/tach_ho_normalized.json",
    f"{DATA_FOLDER}/luu_tru_normalized.json"
]

OUTPUT_FILE = f"{DATA_FOLDER}/all_procedures_normalized.json"

all_chunks = []

# =========================
# G·ªòP D·ªÆ LI·ªÜU
# =========================
for file in FILES:
    with open(file, encoding="utf-8") as f:
        chunks = json.load(f)
        all_chunks.extend(chunks)
        print(f"üì• {file}: {len(chunks)} chunks")

# =========================
# L∆ØU FILE CHUNG
# =========================
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(all_chunks, f, ensure_ascii=False, indent=2)

print("\nüéâ HO√ÄN T·∫§T G·ªòP FILE")
print(f"üì¶ T·ªïng s·ªë chunk: {len(all_chunks)}")
print(f"üìÅ File xu·∫•t ra: {OUTPUT_FILE}")

!pip install -U sentence-transformers torch accelerate

JSON_FILE = [
    "/content/drive/MyDrive/RAG/all_procedures_normalized.json"
]
COLLECTION_NAME = "dichvucong_rag"
EMBEDDING_MODEL = "BAAI/bge-m3"

embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name=EMBEDDING_MODEL
)

client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection(
    name=COLLECTION_NAME,
    embedding_function=embedding_function
)

with open(JSON_FILE[0], "r", encoding="utf-8") as f:
    data = json.load(f)

ids = []
documents = []
metadatas = []

for idx, item in enumerate(data):
    ids.append(item["id"])
    documents.append(item["content_text"])

    meta = item.get("metadata", {}).copy()  # l·∫•y h·∫øt metadata c≈©

    meta.update({
        "url": item["url"],
        "title": item["title"],
        "hierarchy": item["hierarchy"],
        "chunk_type": item["chunk_type"],
        "domain": item["metadata"].get("source_domain", "unknown"),
    })

    metadatas.append(meta)

print(len(data))

collection.add(
    ids=ids,
    documents=documents,
    metadatas=metadatas
)

print(f"ƒê√£ ƒë·∫©y th√†nh c√¥ng {len(data)} chunks v√†o Chroma!")

API_KEY = "AIzaSyC4G5cGpl2XCKmQkAcCl0IEt4tzp_lU3mk"

genai.configure(api_key=API_KEY)
model = genai.GenerativeModel('gemini-2.5-flash')

# ==============================
# TRUY V·∫§N
# ==============================
query = "ƒêƒÉng k√Ω th∆∞·ªùng tr√∫?"

results = collection.query(
    query_texts=[query],
    n_results=6,
    include=["documents", "metadatas", "distances"]
)

# ==============================
# HI·ªÇN TH·ªä C√ÅC CHUNK ƒê∆Ø·ª¢C RETRIEVAL
# ==============================
print("\nüìå K·∫æT QU·∫¢ RETRIEVAL:")
for i, (doc, meta, dist) in enumerate(zip(
        results["documents"][0],
        results["metadatas"][0],
        results["distances"][0]
    ), start=1):

    hierarchy_val = meta.get("hierarchy", meta.get("title", "Kh√¥ng r√µ m·ª•c"))
    url_val = meta.get("url", "Kh√¥ng r√µ ngu·ªìn")

    print(f"\n--- Chunk {i} ---")
    print(f"Distance: {dist}")
    print(f"Hierarchy/Title: {hierarchy_val}")
    print(f"URL: {url_val}")
    print("N·ªôi dung:")
    print(doc)

# ==============================
# T·∫†O CONTEXT CHO PROMPT
# ==============================
context_parts = []
for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
    hierarchy_val = meta.get("hierarchy", meta.get("title", "Kh√¥ng r√µ m·ª•c"))
    url_val = meta.get("url", "Kh√¥ng r√µ ngu·ªìn")

    context_parts.append(
        f"[{hierarchy_val}]\n{doc}\n(Ngu·ªìn: {url_val})"
    )

context = "\n\n".join(context_parts)

# ==============================
# PROMPT RAG (SI·∫æT NGHI√äM)
# ==============================
prompt = f"""
B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh c√¥ng c·ªßa Vi·ªát Nam.

PH·∫†M VI √ÅP D·ª§NG:
- Ch·ªâ t∆∞ v·∫•n c√°c n·ªôi dung LI√äN QUAN ƒê·∫æN th·ªß t·ª•c ƒëƒÉng k√Ω th∆∞·ªùng tr√∫.
- C√°c n·ªôi dung ngo√†i ph·∫°m vi n√†y kh√¥ng thu·ªôc ch·ª©c nƒÉng c·ªßa h·ªá th·ªëng.

NGUY√äN T·∫ÆC B·∫ÆT BU·ªòC:
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong CONTEXT b√™n d∆∞·ªõi.
- Tuy·ªát ƒë·ªëi KH√îNG s·ª≠ d·ª•ng ki·∫øn th·ª©c b√™n ngo√†i.
- KH√îNG suy di·ªÖn, KH√îNG t·ª± b·ªï sung.

C√ÅCH TR·∫¢ L·ªúI:
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ƒë√∫ng tr·ªçng t√¢m c√¢u h·ªèi.
- C√≥ th·ªÉ t·ªïng h·ª£p nhi·ªÅu ƒëo·∫°n trong CONTEXT n·∫øu c√πng m√¥ t·∫£ m·ªôt n·ªôi dung.
- N·∫øu l√† danh s√°ch, tr√¨nh b√†y b·∫±ng g·∫°ch ƒë·∫ßu d√≤ng ho·∫∑c ƒë√°nh s·ªë.

TR∆Ø·ªúNG H·ª¢P KH√îNG ƒê·ª¶ D·ªÆ LI·ªÜU:
N·∫øu CONTEXT kh√¥ng ch·ª©a th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi,
ch·ªâ ƒë∆∞·ª£c tr·∫£ l·ªùi ƒë√∫ng m·ªôt c√¢u sau (kh√¥ng th√™m, kh√¥ng b·ªõt):

"Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong d·ªØ li·ªáu hi·ªán c√≥."

======================
CONTEXT:
{context}
======================
"""

!conda create -n rag_env python=3.10
!conda activate rag_env
!pip install streamlit
!streamlit run app.py

!pip install streamlit
!pip install google-generativeai
!pip install torch torchvision torchaudio
!pip install sentence-transformers
!pip install beautifulsoup4 requests

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import google.generativeai as genai
# genai.configure(api_key="AIzaSyC4G5cGpl2XCKmQkAcCl0IEt4tzp_lU3mk")
# 
# import streamlit as st
# import json
# import os
# import chromadb
# from chromadb.utils import embedding_functions
# from sentence_transformers import SentenceTransformer
# import google.generativeai as genai
# 
# # Configure Gemini API key (thay b·∫±ng key th·∫≠t c·ªßa b·∫°n t·ª´ Google AI Studio)
# genai.configure(api_key="AIzaSyC4G5cGpl2XCKmQkAcCl0IEt4tzp_lU3mk")
# 
# # ================= C·∫§U H√åNH =================
# JSON_FILE = "/content/drive/MyDrive/RAG/all_procedures_normalized.json"  # ƒê∆∞·ªùng d·∫´n file JSON (sau chunk rule-based)
# CHROMA_DB_PATH = "chroma_db"
# COLLECTION_NAME = "dichvucong_rag"
# GEMINI_MODEL = "gemini-2.5-flash"
# 
# @st.cache_resource
# def get_embedding_function():
#     EMBEDDING_MODEL = "BAAI/bge-m3"  # Model embedding ti·∫øng Vi·ªát
#     embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=EMBEDDING_MODEL)
#     return embedding_function
# 
# @st.cache_resource
# def load_collection():
#     chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
#     embedding_func = get_embedding_function()
# 
#     try:
#         collection = chroma_client.get_collection(
#             name=COLLECTION_NAME,
#             embedding_function=embedding_func  # c·∫ßn ƒë·ªÉ query ƒë√∫ng
#         )
#         st.success(f"Collection '{COLLECTION_NAME}' ƒë√£ load t·ª´ {CHROMA_DB_PATH}")
#     except Exception as e:
#         st.error(f"Kh√¥ng t√¨m th·∫•y collection '{COLLECTION_NAME}' trong {CHROMA_DB_PATH}: {e}")
#         collection = None
# 
#     return collection
# # --- Load collection 1 l·∫ßn ---
# collection = load_collection()
# 
# def query_rag(query: str, chat_history: list, top_k: int):
#     # Retrieval v·ªõi top_k ƒë·ªông
#     results = collection.query(
#         query_texts=[query],
#         n_results=top_k,
#         include=["documents", "metadatas", "distances"]
#     )
# 
#     context_parts = []
#     for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
#         context_parts.append(f"[{meta['hierarchy']}]\\n{doc}\\n(Ngu·ªìn: {meta['url']})")
# 
#     context = "\\n\\n".join(context_parts)
# 
#     prompt = f"""
#     B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh ƒëƒÉng k√Ω th∆∞·ªùng tr√∫ t·∫°i Vi·ªát Nam. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c, d·ªÖ hi·ªÉu, c√≥ d·∫´n ngu·ªìn. Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n context, kh√¥ng th√™m th√¥ng tin ngo√†i.
# 
#     Context:
#     {context}
# 
#     C√¢u h·ªèi: {query}
# 
#     Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, c√≥ ƒë√°nh s·ªë n·∫øu l√† danh s√°ch, v√† tr√≠ch d·∫´n ngu·ªìn r√µ r√†ng (t√™n block, URL):
#     """
# 
#     model = genai.GenerativeModel(GEMINI_MODEL)
#     response = model.generate_content(prompt, stream=True)
# 
#     return response
# 
# # Giao di·ªán ch√≠nh
# st.set_page_config(page_title="Chatbot t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh ƒëƒÉng k√Ω th∆∞·ªùng tr√∫", layout="centered")
# st.title("Chatbot t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh ƒëƒÉng k√Ω th∆∞·ªùng tr√∫")
# 
# # Sidebar v·ªõi top-k slider v√† th√¥ng tin
# with st.sidebar:
#     st.header("C√†i ƒë·∫∑t")
#     top_k = st.slider("Top-k retrieval (s·ªë chunks l·∫•y v·ªÅ)", min_value=1, max_value=10, value=3, step=1)
#     st.header("Th√¥ng tin")
#     st.write(f"Vector DB: {COLLECTION_NAME}")
#     st.write(f"S·ªë chunk: {collection.count() if collection else 0}")
#     st.write(f"Model LLM: {GEMINI_MODEL}")
#     st.write("Embedding: BAAI/bge-m3 (t·ªëi ∆∞u ti·∫øng Vi·ªát)")
#     st.caption("D·ªØ li·ªáu load t·ª´ file JSON. N·∫øu l·ªói, ki·ªÉm tra ƒë∆∞·ªùng d·∫´n JSON_FILE.")
# 
# # Kh·ªüi t·∫°o l·ªãch s·ª≠ chat
# if "messages" not in st.session_state:
#     st.session_state.messages = []
# 
# # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
# for message in st.session_state.messages:
#     with st.chat_message(message["role"]):
#         st.markdown(message["content"])
# 
# # Input t·ª´ user
# if prompt := st.chat_input("H·ªèi v·ªÅ ƒëƒÉng k√Ω th∆∞·ªùng tr√∫ (v√≠ d·ª•: ƒêƒÉng k√Ω th∆∞·ªùng tr√∫ c·∫ßn g√¨?)"):
#     # Th√™m tin nh·∫Øn user
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(prompt)
# 
#     # G·ªçi RAG v·ªõi top_k t·ª´ slider v√† stream response
#     with st.chat_message("assistant"):
#         message_placeholder = st.empty()
#         full_response = ""
# 
#         try:
#             response = query_rag(prompt, st.session_state.messages, top_k)
#             for chunk in response:
#                 if chunk.text:
#                     full_response += chunk.text
#                     message_placeholder.markdown(full_response + " ")
#             message_placeholder.markdown(full_response)
#         except Exception as e:
#             full_response = f"L·ªói khi g·ªçi Gemini: {str(e)}"
#             message_placeholder.error(full_response)
# 
#     # L∆∞u response v√†o l·ªãch s·ª≠
#     st.session_state.messages.append({"role": "assistant", "content": full_response})
# # D√°n to√†n b·ªô code tr√™n v√†o ƒë√¢y
#

!sed -n '1,200p' app.py

!pip install openai streamlit

# ƒê·ªÉ kh·∫Øc ph·ª•c l·ªói StreamlitSecretNotFoundError:
!mkdir -p .streamlit
!echo 'OPENAI_API_KEY="AIzaSyC4G5cGpl2XCKmQkAcCl0IEt4tzp_lU3mk"' > .streamlit/secrets.toml
#    (Nh·ªõ thay YOUR_API_KEY b·∫±ng kh√≥a OpenAI API th·ª±c t·∫ø c·ªßa b·∫°n)
# D√≤ng n√†y ch·ªâ l√† h∆∞·ªõng d·∫´n, kh√¥ng ph·∫£i code ƒë·ªÉ ch·∫°y.

import streamlit as st
from openai import OpenAI

st.title("ChatGPT-like clone")

# Set OpenAI API key from Streamlit secrets
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# Set a default model
if "openai_model" not in st.session_state:
    st.session_state["openai_model"] = "gpt-3.5-turbo"

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat messages from history on app rerun
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Accept user input
if prompt := st.chat_input("What is up?"):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    # Display user message in chat message container
    with st.chat_message("user"):
        st.markdown(prompt)

    # Display assistant response in chat message container
    with st.chat_message("assistant"):
        stream = client.chat.completions.create(
            model=st.session_state["openai_model"],
            messages=[
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.messages
            ],
            stream=True,
        )
        response = st.write_stream(stream)
    st.session_state.messages.append({"role": "assistant", "content": response})

!npm install localtunnel

!streamlit run app.py &>/content/logs.txt &
!npx localtunnel --port 8501 & curl ipv4.icanhazip.com

query ="C√≥ bao nhi√™u c√°ch th·ª±c hi·ªán ƒëƒÉng k√Ω th∆∞·ªùng tr√∫?"

results = collection.query(
        query_texts=[query],
        n_results=6,
        include=["documents", "metadatas", "distances"]
    )

# ==============================
# HI·ªÇN TH·ªä C√ÅC CHUNK ƒê∆Ø·ª¢C RETRIEVAL
# ==============================
print("\nüìå K·∫æT QU·∫¢ RETRIEVAL:")
for i, (doc, meta, dist) in enumerate(zip(
        results["documents"][0],
        results["metadatas"][0],
        results["distances"][0]
    ), start=1):

    hierarchy_val = meta.get('hierarchy', meta.get('title', 'Kh√¥ng r√µ m·ª•c'))
    url_val = meta.get('url', 'Kh√¥ng r√µ ngu·ªìn')

    print(f"\n--- Chunk {i} ---")
    print(f"Distance: {dist}")
    print(f"Hierarchy/Title: {hierarchy_val}")
    print(f"URL: {url_val}")
    print("N·ªôi dung:")
    print(doc)

# ==============================
# T·∫†O CONTEXT CHO PROMPT
# ==============================
context_parts = []
for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
    hierarchy_val = meta.get('hierarchy', meta.get('title', 'Kh√¥ng r√µ m·ª•c'))
    url_val = meta.get('url', 'Kh√¥ng r√µ ngu·ªìn')
    context_parts.append(f"[{hierarchy_val}]\n{doc}\n(Ngu·ªìn: {url_val})")

context = "\n\n".join(context_parts)

prompt = f"""
B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh c√¥ng Vi·ªát Nam.

PH·∫†M VI √ÅP D·ª§NG:
- ∆Øu ti√™n t∆∞ v·∫•n c√°c th·ªß t·ª•c h√†nh ch√≠nh li√™n quan ƒë·∫øn h·ªó tr·ª£ c∆∞ tr√∫.
- N·∫øu CONTEXT kh√¥ng ƒë·ªÅ c·∫≠p r√µ ƒë·ªô tu·ªïi nh∆∞ng n·ªôi dung thu·ªôc th·ªß t·ª•c th∆∞·ªùng √°p d·ª•ng cho tr·∫ª em,
  b·∫°n ƒë∆∞·ª£c ph√©p tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin hi·ªán c√≥ v√† n√™u r√µ ph·∫°m vi √°p d·ª•ng n·∫øu ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p.

NGUY√äN T·∫ÆC TR·∫¢ L·ªúI:
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin c√≥ trong CONTEXT b√™n d∆∞·ªõi.
- Kh√¥ng s·ª≠ d·ª•ng ki·∫øn th·ª©c b√™n ngo√†i.
- Kh√¥ng t·ª± b·ªï sung th√¥ng tin kh√¥ng c√≥ trong CONTEXT.

C√ÅCH TR·∫¢ L·ªúI:
- Ch·ªâ tr·∫£ l·ªùi c√°c n·ªôi dung LI√äN QUAN TR·ª∞C TI·∫æP ƒë·∫øn c√¢u h·ªèi.
- C√≥ th·ªÉ t·ªïng h·ª£p nhi·ªÅu ƒëo·∫°n trong CONTEXT n·∫øu ch√∫ng c√πng m√¥ t·∫£ m·ªôt th·ªß t·ª•c.
- Tr√¨nh b√†y ng·∫Øn g·ªçn, r√µ r√†ng, ƒë√∫ng tr·ªçng t√¢m.

TR∆Ø·ªúNG H·ª¢P KH√îNG TR·∫¢ L·ªúI:
Ch·ªâ tr·∫£ l·ªùi ƒë√∫ng c√¢u sau n·∫øu:
- CONTEXT ho√†n to√†n kh√¥ng ch·ª©a th√¥ng tin li√™n quan ƒë·∫øn c√¢u h·ªèi.

C√¢u tr·∫£ l·ªùi trong tr∆∞·ªùng h·ª£p n√†y PH·∫¢I CH√çNH X√ÅC:
"Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong d·ªØ li·ªáu hi·ªán c√≥."

Y√äU C·∫¶U ƒê·ªäNH D·∫†NG:
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.
- N·∫øu c√≥ nhi·ªÅu √Ω, tr√¨nh b√†y b·∫±ng g·∫°ch ƒë·∫ßu d√≤ng ho·∫∑c ƒë√°nh s·ªë.
- Gi·ªØ nguy√™n tr√≠ch d·∫´n ngu·ªìn n·∫øu c√≥ trong CONTEXT.

======================
CONTEXT:
{context}
======================

C√ÇU H·ªéI:
{query}

C√ÇU TR·∫¢ L·ªúI:
"""

try:
    response = model.generate_content(prompt)
    print("\nüìù TR·∫¢ L·ªúI C·ª¶A H·ªÜ TH·ªêNG:")
    print(response.text)
except Exception as e:
    error_msg = str(e).lower()

    if "quota" in error_msg or "limit" in error_msg or "exceeded" in error_msg or "429" in error_msg:
        print("‚ö†Ô∏è Phi√™n l√†m vi·ªác hi·ªán t·∫°i ƒë√£ k·∫øt th√∫c do ƒë√£ s·ª≠ d·ª•ng h·∫øt s·ªë l∆∞·ª£t c√¢u h·ªèi. Vui l√≤ng ch·ªù sang phi√™n ti·∫øp theo ƒë·ªÉ ti·∫øp t·ª•c tra c·ª©u th√¥ng tin.")
    else:
        print("‚ö†Ô∏è H·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë khi x·ª≠ l√Ω c√¢u h·ªèi. Vui l√≤ng th·ª≠ l·∫°i sau.")

!conda create -n rag_env python=3.10
!conda activate rag_env
!pip install streamlit
!streamlit run app.py

!pip install streamlit
!pip install google-generativeai
!pip install torch torchvision torchaudio
!pip install sentence-transformers
!pip install beautifulsoup4 requests

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import google.generativeai as genai
# genai.configure(api_key="AIzaSyC4G5cGpl2XCKmQkAcCl0IEt4tzp_lU3mk")
# 
# import streamlit as st
# import json
# import os
# import chromadb
# from chromadb.utils import embedding_functions
# from sentence_transformers import SentenceTransformer
# import google.generativeai as genai
# 
# # Configure Gemini API key (thay b·∫±ng key th·∫≠t c·ªßa b·∫°n t·ª´ Google AI Studio)
# genai.configure(api_key="AIzaSyC4G5cGpl2XCKmQkAcCl0IEt4tzp_lU3mk")
# 
# # ================== C·∫§U H√åNH ==================
# JSON_FILE = "/content/drive/MyDrive/RAG/all_procedures_normalized.json"  # ƒê∆∞·ªùng d·∫´n file JSON (sau chunk rule-based)
# CHROMA_DB_PATH = "chroma_db"  # Th∆∞ m·ª•c l∆∞u vector DB
# COLLECTION_NAME = "dichvucong_rag"
# GEMINI_MODEL = "gemini-2.5-flash"  # Ho·∫∑c "gemini-1.5-pro"
# 
# @st.cache_resource
# def get_embedding_function():
#     EMBEDDING_MODEL = "BAAI/bge-m3"  # Model embedding ti·∫øng Vi·ªát
#     embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=EMBEDDING_MODEL)
#     return embedding_function
# 
# @st.cache_resource
# def load_collection():
#     chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
#     embedding_func = get_embedding_function()
# 
#     try:
#         collection = chroma_client.get_collection(
#             name=COLLECTION_NAME,
#             embedding_function=embedding_func  # c·∫ßn ƒë·ªÉ query ƒë√∫ng
#         )
#         st.success(f"Collection '{COLLECTION_NAME}' ƒë√£ load t·ª´ {CHROMA_DB_PATH}")
#     except Exception as e:
#         st.error(f"Kh√¥ng t√¨m th·∫•y collection '{COLLECTION_NAME}' trong {CHROMA_DB_PATH}: {e}")
#         collection = None
# 
#     return collection
# # --- Load collection 1 l·∫ßn ---
# collection = load_collection()
# 
# def query_rag(query: str, chat_history: list, top_k: int):
#     # Retrieval v·ªõi top_k ƒë·ªông
#     results = collection.query(
#         query_texts=[query],
#         n_results=top_k,
#         include=["documents", "metadatas", "distances"]
#     )
# 
#     context_parts = []
#     for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
#         context_parts.append(f"[{meta['hierarchy']}]\\n{doc}\\n(Ngu·ªìn: {meta['url']})")
# 
#     context = "\\n\\n".join(context_parts)
# 
#     prompt = f"""
#     B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh v·ªÅ c∆∞ tr√∫ t·∫°i Vi·ªát Nam. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c, d·ªÖ hi·ªÉu, c√≥ d·∫´n ngu·ªìn. Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n context, kh√¥ng th√™m th√¥ng tin ngo√†i.
# 
#     Context:
#     {context}
# 
#     C√¢u h·ªèi: {query}
# 
#     Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, c√≥ ƒë√°nh s·ªë n·∫øu l√† danh s√°ch, v√† tr√≠ch d·∫´n ngu·ªìn r√µ r√†ng (t√™n block, URL):
#     """
# 
#     model = genai.GenerativeModel(GEMINI_MODEL)
#     response = model.generate_content(prompt, stream=True)
# 
#     return response
# 
# # Giao di·ªán ch√≠nh
# st.set_page_config(page_title="Chatbot t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh v·ªÅ c∆∞ tr√∫", layout="centered")
# st.title("Chatbot t∆∞ v·∫•n th·ªß t·ª•c h√†nh ch√≠nh v·ªÅ c∆∞ tr√∫")
# 
# # Sidebar v·ªõi top-k slider v√† th√¥ng tin
# with st.sidebar:
#     st.header("C√†i ƒë·∫∑t")
#     top_k = st.slider("Top-k retrieval (s·ªë chunks l·∫•y v·ªÅ)", min_value=1, max_value=10, value=3, step=1)
#     st.header("Th√¥ng tin")
#     st.write(f"Vector DB: {COLLECTION_NAME}")
#     st.write(f"S·ªë chunk: {collection.count() if collection else 0}")
#     st.write(f"Model LLM: {GEMINI_MODEL}")
#     st.write("Embedding: BAAI/bge-m3 (t·ªëi ∆∞u ti·∫øng Vi·ªát)")
#     st.caption("D·ªØ li·ªáu load t·ª´ file JSON. N·∫øu l·ªói, ki·ªÉm tra ƒë∆∞·ªùng d·∫´n JSON_FILE.")
# 
# # Kh·ªüi t·∫°o l·ªãch s·ª≠ chat
# if "messages" not in st.session_state:
#     st.session_state.messages = []
# 
# # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
# for message in st.session_state.messages:
#     with st.chat_message(message["role"]):
#         st.markdown(message["content"])
# 
# # Input t·ª´ user
# if prompt := st.chat_input("H·ªèi v·ªÅ th·ªß t·ª•c h√†nh ch√≠nh v·ªÅ c∆∞ tr√∫ (v√≠ d·ª•: ƒêƒÉng k√Ω th∆∞·ªùng tr√∫ c·∫ßn g√¨?)"):
#     # Th√™m tin nh·∫Øn user
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(prompt)
# 
#     # G·ªçi RAG v·ªõi top_k t·ª´ slider v√† stream response
#     with st.chat_message("assistant"):
#         message_placeholder = st.empty()
#         full_response = ""
# 
#         try:
#             response = query_rag(prompt, st.session_state.messages, top_k)
#             for chunk in response:
#                 if chunk.text:
#                     full_response += chunk.text
#                     message_placeholder.markdown(full_response + " ")
#             message_placeholder.markdown(full_response)
#         except Exception as e:
#             full_response = f"L·ªói khi g·ªçi Gemini: {str(e)}"
#             message_placeholder.error(full_response)
# 
#     # L∆∞u response v√†o l·ªãch s·ª≠
#     st.session_state.messages.append({"role": "assistant", "content": full_response})
# # D√°n to√†n b·ªô code tr√™n v√†o ƒë√¢y

!sed -n '1,200p' app.py

!pip install openai streamlit

# ƒê·ªÉ kh·∫Øc ph·ª•c l·ªói StreamlitSecretNotFoundError:
!mkdir -p .streamlit
!echo 'OPENAI_API_KEY="AIzaSyC4G5cGpl2XCKmQkAcCl0IEt4tzp_lU3mk"' > .streamlit/secrets.toml
#    (Nh·ªõ thay YOUR_API_KEY b·∫±ng kh√≥a OpenAI API th·ª±c t·∫ø c·ªßa b·∫°n)
# D√≤ng n√†y ch·ªâ l√† h∆∞·ªõng d·∫´n, kh√¥ng ph·∫£i code ƒë·ªÉ ch·∫°y.

import streamlit as st
from openai import OpenAI

st.title("ChatGPT-like clone")

# Set OpenAI API key from Streamlit secrets
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# Set a default model
if "openai_model" not in st.session_state:
    st.session_state["openai_model"] = "gpt-3.5-turbo"

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat messages from history on app rerun
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Accept user input
if prompt := st.chat_input("What is up?"):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    # Display user message in chat message container
    with st.chat_message("user"):
        st.markdown(prompt)

    # Display assistant response in chat message container
    with st.chat_message("assistant"):
        stream = client.chat.completions.create(
            model=st.session_state["openai_model"],
            messages=[
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.messages
            ],
            stream=True,
        )
        response = st.write_stream(stream)
    st.session_state.messages.append({"role": "assistant", "content": response})

!npm install localtunnel

!streamlit run app.py &>/content/logs.txt &
!npx localtunnel --port 8501 & curl ipv4.icanhazip.com

"""# Task
The `FILES` list in cell `LWEIZI6G2-Bc` contains extra closing parentheses at the end of each file path string, causing a `SyntaxError`. I will remove these extra parentheses and then re-execute the cell to correctly merge the normalized JSON files.

```python
FILES = [
    f"{DATA_FOLDER}/thuong_tru_normalized.json",
    f"{DATA_FOLDER}/tam_tru_normalized.json",
    f"{DATA_FOLDER}/thuong_tru_bca_normalized.json",
    f"{DATA_FOLDER}/tam_tru_bca_normalized.json",
    f"{DATA_FOLDER}/khai_bao_thong_tin_normalized.json",
    f"{DATA_FOLDER}/dieu_chinh_thong_tin_cu_tru_normalized.json",
    f"{DATA_FOLDER}/tam_vang_normalized.json",
    f"{DATA_FOLDER}/tach_ho_normalized.json",
    f"{DATA_FOLDER}/luu_tru_normalized.json"
]

OUTPUT_FILE = f"{DATA_FOLDER}/all_procedures_normalized.json"

all_chunks = []

# =========================
# G·ªòP D·ªÆ LI·ªÜU
# =========================
for file in FILES:
    with open(file, encoding="utf-8") as f:
        chunks = json.load(f)
        all_chunks.extend(chunks)
        print(f"üì• {file}: {len(chunks)} chunks")

# =========================
# L∆ØU FILE CHUNG
# =========================
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(all_chunks, f, ensure_ascii=False, indent=2)

print("\nüéâ HO√ÄN T·∫§T G·ªòP FILE")
print(f"üì¶ T·ªïng s·ªë chunk: {len(all_chunks)}")
print(f"üìÅ File xu·∫•t ra: {OUTPUT_FILE}")
```

## Remove extra parentheses in FILES list

### Subtask:
Correct the `FILES` list in cell `LWEIZI6G2-Bc` by removing extraneous closing parentheses from each file path string.

**Reasoning**:
The subtask requires correcting the `FILES` list in cell `LWEIZI6G2-Bc` by removing extraneous closing parentheses. I will generate a code block to update the `FILES` list with the corrected file paths.
"""

FILES = [
    f"{DATA_FOLDER}/thuong_tru_normalized.json",
    f"{DATA_FOLDER}/tam_tru_normalized.json",
    f"{DATA_FOLDER}/thuong_tru_bca_normalized.json",
    f"{DATA_FOLDER}/tam_tru_bca_normalized.json",
    f"{DATA_FOLDER}/khai_bao_thong_tin_normalized.json",
    f"{DATA_FOLDER}/dieu_chinh_thong_tin_cu_tru_normalized.json",
    f"{DATA_FOLDER}/tam_vang_normalized.json",
    f"{DATA_FOLDER}/tach_ho_normalized.json",
    f"{DATA_FOLDER}/luu_tru_normalized.json"
]

OUTPUT_FILE = f"{DATA_FOLDER}/all_procedures_normalized.json"

all_chunks = []

# =========================
# G·ªòP D·ªÆ LI·ªÜU
# =========================
for file in FILES:
    with open(file, encoding="utf-8") as f:
        chunks = json.load(f)
        all_chunks.extend(chunks)
        print(f"üì• {file}: {len(chunks)} chunks")

# =========================
# L∆ØU FILE CHUNG
# =========================
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
    json.dump(all_chunks, f, ensure_ascii=False, indent=2)

print("\nüéâ HO√ÄN T·∫§T G·ªòP FILE")
print(f"üì¶ T·ªïng s·ªë chunk: {len(all_chunks)}")
print(f"üìÅ File xu·∫•t ra: {OUTPUT_FILE}")

"""## Summary:

### Data Analysis Key Findings
*   The initial `SyntaxError` was caused by extraneous closing parentheses in the file path strings within the `FILES` list.
*   The issue was resolved by removing these extra parentheses, allowing the script to execute successfully.
*   Nine normalized JSON files were successfully processed and merged:
    *   `thuong_tru_normalized.json`: 7 chunks
    *   `tam_tru_normalized.json`: 21 chunks
    *   `thuong_tru_bca_normalized.json`: 30 chunks
    *   `tam_tru_bca_normalized.json`: 32 chunks
    *   `khai_bao_thong_tin_normalized.json`: 70 chunks
    *   `dieu_chinh_thong_tin_cu_tru_normalized.json`: 43 chunks
    *   `tam_vang_normalized.json`: 31 chunks
    *   `tach_ho_normalized.json`: 10 chunks
    *   `luu_tru_normalized.json`: 1069 chunks
*   A total of 1313 chunks were merged into the output file `all_procedures_normalized.json`.

### Insights or Next Steps
*   The successfully merged data in `all_procedures_normalized.json` is now ready for subsequent analysis or processing steps.
*   Implementing a more robust input validation or sanitization process for file paths could prevent similar `SyntaxError` issues in the future.

"""