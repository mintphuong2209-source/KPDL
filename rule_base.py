# ================= 1. FIX L·ªñI SQLITE (B·∫ÆT BU·ªòC TR√äN STREAMLIT CLOUD) =================
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# ======================================================================================

import streamlit as st
import json
import os
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# ================= C·∫§U H√åNH TRANG (ƒê√ÅP ·ª®NG C√ÇU 5) =================
st.set_page_config(page_title="Chatbot H·ªó Tr·ª£ C∆∞ Tr√∫", page_icon="üáªüá≥", layout="wide")
st.title("ü§ñ Tr·ª£ L√Ω ·∫¢o T∆∞ V·∫•n Ph√°p Lu·∫≠t C∆∞ Tr√∫")
st.markdown("---")

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n file data
JSON_FILE = "all_chunks_normalized.json"
COLLECTION_NAME = "dichvucong_rag_v1"

# ================= 2. C·∫§U H√åNH API (ƒê√ÅP ·ª®NG C√ÇU 4) =================
with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh")
    # L·∫•y API Key t·ª´ Secrets ho·∫∑c nh·∫≠p tay
    api_key = st.secrets.get("GEMINI_API_KEY")
    if not api_key:
        api_key = st.text_input("Nh·∫≠p Google AI API Key:", type="password")
    
    if not api_key:
        st.warning("üëâ Vui l√≤ng nh·∫≠p API Key ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
        st.stop()
    
    genai.configure(api_key=api_key)
    
    # Cho ph√©p ch·ªânh s·ªë l∆∞·ª£ng chunk retrieval (Advanced)
    top_k = st.slider("S·ªë l∆∞·ª£ng ngu·ªìn tham kh·∫£o (Top-K):", 1, 10, 5)

# ================= 3. H√ÄM N·∫†P D·ªÆ LI·ªÜU (OPTIMIZED) =================
@st.cache_resource(show_spinner=False)
def load_vector_db():
    try:
        # S·ª≠ d·ª•ng model embedding t·ªëi ∆∞u cho ti·∫øng Vi·ªát (nh·∫π h∆°n bge-m3 ƒë·ªÉ ch·∫°y cloud)
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="keepitreal/vietnamese-sbert"
        )
        
        # T·∫°o Client (Ephemeral - ch·∫°y tr√™n RAM ƒë·ªÉ t·ªëc ƒë·ªô cao nh·∫•t)
        chroma_client = chromadb.Client()
        collection = chroma_client.get_or_create_collection(
            name=COLLECTION_NAME,
            embedding_function=embedding_function
        )

        # Ki·ªÉm tra n·∫øu DB r·ªóng th√¨ n·∫°p t·ª´ file JSON
        if collection.count() == 0:
            if not os.path.exists(JSON_FILE):
                return None, f"Kh√¥ng t√¨m th·∫•y file {JSON_FILE}!"

            with open(JSON_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)

            ids = []
            documents = []
            metadatas = []

            # Thanh ti·∫øn tr√¨nh
            progress_bar = st.progress(0, text="ƒêang n·∫°p d·ªØ li·ªáu tri th·ª©c...")
            total = len(data)

            for idx, item in enumerate(data):
                # L·ªçc b·ªè chunk r√°c
                content = item.get("content_text", "").strip()
                if len(content) < 10: continue

                ids.append(str(item.get("id", idx)))
                documents.append(content)
                
                # X·ª≠ l√Ω Metadata k·ªπ c√†ng ƒë·ªÉ hi·ªÉn th·ªã ngu·ªìn (C√¢u 5)
                meta_source = item.get("metadata", {})
                clean_meta = {
                    "url": str(item.get("url", "#")),
                    "hierarchy": str(item.get("hierarchy", "Th√¥ng tin chung")),
                    "title": str(item.get("title", "")),
                    "source_domain": str(meta_source.get("source_domain", "DVCQG"))
                }
                metadatas.append(clean_meta)
                
                # C·∫≠p nh·∫≠t thanh ti·∫øn tr√¨nh m·ªói 10%
                if idx % (total // 10 + 1) == 0:
                    progress_bar.progress(idx / total)

            # N·∫°p v√†o Chroma (Batch size 100)
            batch_size = 100
            for i in range(0, len(ids), batch_size):
                collection.add(
                    ids=ids[i:i+batch_size],
                    documents=documents[i:i+batch_size],
                    metadatas=metadatas[i:i+batch_size]
                )
            
            progress_bar.empty() # X√≥a thanh ti·∫øn tr√¨nh khi xong
            return collection, f"ƒê√£ n·∫°p m·ªõi {len(ids)} chunks."
        
        return collection, f"S·∫µn s√†ng ({collection.count()} chunks)."

    except Exception as e:
        return None, str(e)

# --- KH·ªûI ƒê·ªòNG DATABASE ---
with st.spinner("üöÄ ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng..."):
    collection, status_msg = load_vector_db()

if not collection:
    st.error(f"‚ùå L·ªói h·ªá th·ªëng: {status_msg}")
    st.stop()
else:
    st.sidebar.success(f"üì¶ Tr·∫°ng th√°i: {status_msg}")

# ================= 4. LOGIC RAG & GEMINI (C√ÇU 4) =================
def query_gemini(question, k_neighbors):
    # 1. Retrieval
    results = collection.query(
        query_texts=[question],
        n_results=k_neighbors
    )
    
    context_parts = []
    sources_debug = [] # ƒê·ªÉ hi·ªÉn th·ªã metadata chi ti·∫øt
    
    if results['documents']:
        for i, doc in enumerate(results['documents'][0]):
            meta = results['metadatas'][0][i]
            # Format: [Ngu·ªìn] N·ªôi dung
            source_info = f"[{meta['hierarchy']}]"
            context_parts.append(f"{source_info}\n{doc}")
            
            # L∆∞u l·∫°i ƒë·ªÉ hi·ªÉn th·ªã UI
            sources_debug.append({
                "title": meta['hierarchy'],
                "url": meta['url'],
                "content": doc,
                "domain": meta['source_domain']
            })
            
    context_text = "\n\n".join(context_parts)
    
    # 2. Generation (Prompt Engineering)
    prompt = f"""
    B·∫°n l√† tr·ª£ l√Ω ·∫£o ph√°p lu·∫≠t chuy√™n v·ªÅ C∆∞ tr√∫.
    D·ª±a v√†o c√°c ƒëo·∫°n vƒÉn b·∫£n ph√°p lu·∫≠t ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa c√¥ng d√¢n.
    
    Y√äU C·∫¶U:
    1. Tr·∫£ l·ªùi ch√≠nh x√°c, ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu.
    2. D·∫´n ch·ª©ng th√¥ng tin l·∫•y t·ª´ m·ª•c n√†o (d·ª±a v√†o ph·∫ßn [...] trong context).
    3. N·∫øu kh√¥ng c√≥ th√¥ng tin trong ng·ªØ c·∫£nh, h√£y n√≥i "T√¥i ch∆∞a t√¨m th·∫•y th√¥ng tin trong d·ªØ li·ªáu hi·ªán c√≥".
    
    NG·ªÆ C·∫¢NH (CONTEXT):
    {context_text}
    
    C√ÇU H·ªéI: {question}
    """
    
    try:
        model = genai.GenerativeModel("gemini-2.5-flash") # Th·ª≠ g·ªçi b·∫£n m·ªõi nh·∫•t
        response = model.generate_content(prompt)
        return response.text, sources_debug
    except:
        # Fallback v·ªÅ b·∫£n ·ªïn ƒë·ªãnh n·∫øu b·∫£n 2.5 l·ªói
        try:
            model = genai.GenerativeModel("gemini-1.5-flash")
            response = model.generate_content(prompt)
            return response.text, sources_debug
        except Exception as e:
            return f"‚ö†Ô∏è L·ªói k·∫øt n·ªëi AI: {str(e)}", []

# ================= 5. GIAO DI·ªÜN CHAT (C√ÇU 5) =================
# Kh·ªüi t·∫°o l·ªãch s·ª≠ chat
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ v·ªÅ th·ªß t·ª•c ƒëƒÉng k√Ω th∆∞·ªùng tr√∫, t·∫°m tr√∫, hay t√°ch h·ªô?"}]

# Hi·ªÉn th·ªã l·ªãch s·ª≠
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        # N·∫øu c√≥ th√¥ng tin debug (ngu·ªìn) t·ª´ tin nh·∫Øn c≈© th√¨ hi·ªÉn th·ªã l·∫°i
        if "sources" in msg:
            with st.expander("üîç Xem ngu·ªìn & Metadata (Evidence)"):
                for s in msg["sources"]:
                    st.markdown(f"**üìë {s['title']}**")
                    st.caption(f"Ngu·ªìn: {s['domain']} | [Xem chi ti·∫øt]({s['url']})")
                    st.text(f"{s['content'][:150]}...") # Tr√≠ch d·∫´n 1 ƒëo·∫°n ng·∫Øn
                    st.divider()

# X·ª≠ l√Ω input m·ªõi
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").markdown(prompt)
    
    with st.chat_message("assistant"):
        with st.spinner("ƒêang tra c·ª©u vƒÉn b·∫£n ph√°p lu·∫≠t..."):
            answer, sources = query_gemini(prompt, top_k)
            
            st.markdown(answer)
            
            # Hi·ªÉn th·ªã ngu·ªìn chi ti·∫øt (ƒÇn ƒëi·ªÉm c√¢u hi·ªÉn th·ªã metadata/chunk)
            if sources:
                with st.expander("üîç Xem ngu·ªìn & Metadata (Evidence)"):
                    st.info("C√°c ƒëo·∫°n vƒÉn b·∫£n ƒë∆∞·ª£c AI s·ª≠ d·ª•ng ƒë·ªÉ tr·∫£ l·ªùi:")
                    for s in sources:
                        st.markdown(f"**üìë {s['title']}**")
                        st.caption(f"Ngu·ªìn: {s['domain']} | [Xem chi ti·∫øt]({s['url']})")
                        st.text(f"N·ªôi dung chunk: {s['content'][:200]}...") # Show 200 k√Ω t·ª± ƒë·∫ßu
                        st.divider()
            
            # L∆∞u v√†o l·ªãch s·ª≠ k√®m ngu·ªìn
            st.session_state.messages.append({
                "role": "assistant",
                "content": answer,
                "sources": sources
            })
