__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# =============================================================================

import streamlit as st
import json
import os
import glob # Th∆∞ vi·ªán ƒë·ªÉ t√¨m t·∫•t c·∫£ file
import uuid
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# ================= C·∫§U H√åNH TRANG =================
st.set_page_config(page_title="Chatbot T·ªïng H·ª£p C∆∞ Tr√∫", layout="wide")
st.title("ü§ñ Chatbot T∆∞ V·∫•n Ph√°p Lu·∫≠t & C∆∞ Tr√∫")

COLLECTION_NAME = "all_files_db_v1" # T√™n kho d·ªØ li·ªáu chung

# ================= 2. C·∫§U H√åNH API & MODEL =================
st.sidebar.header("‚öôÔ∏è C·∫•u h√¨nh")

api_key = st.secrets.get("GEMINI_API_KEY") 
if not api_key:
    api_key = st.sidebar.text_input("Nh·∫≠p Google AI Studio API Key:", type="password")
    if not api_key:
        st.warning("üëâ Vui l√≤ng nh·∫≠p API Key ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
        st.stop()

genai.configure(api_key=api_key)

# T·ª± ƒë·ªông ch·ªçn model (∆Øu ti√™n Flash)
try:
    models = [m.name for m in genai.list_models() if 'gemini' in m.name and 'generateContent' in m.supported_generation_methods]
    default_idx = 0
    for i, m in enumerate(models):
        if "flash" in m: default_idx = i; break
    
    SELECTED_MODEL = st.sidebar.selectbox("Ch·ªçn Model AI:", models, index=default_idx)
except Exception as e:
    st.sidebar.error(f"L·ªói k·∫øt n·ªëi API: {e}")
    st.stop()

# ================= 3. H√ÄM LOAD TO√ÄN B·ªò FILE JSON =================
@st.cache_resource(ttl="2h") 
def load_all_json_files():
    """
    H√†m n√†y qu√©t to√†n b·ªô file .json trong th∆∞ m·ª•c v√† n·∫°p v√†o ChromaDB.
    Tuy·ªát ƒë·ªëi KH√îNG v·∫Ω UI trong h√†m n√†y ƒë·ªÉ tr√°nh l·ªói Cache.
    """
    EMBEDDING_MODEL = "keepitreal/vietnamese-sbert"
    
    try:
        # 1. Kh·ªüi t·∫°o ChromaDB
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=EMBEDDING_MODEL
        )
        chroma_client = chromadb.Client()
        collection = chroma_client.get_or_create_collection(
            name=COLLECTION_NAME,
            embedding_function=embedding_function
        )
        
        # 2. Ch·ªâ n·∫°p n·∫øu DB r·ªóng
        if collection.count() == 0:
            # --- T·ª∞ ƒê·ªòNG T√åM FILE ---
            json_files = glob.glob("*.json") # L·∫•y t·∫•t c·∫£ file c√≥ ƒëu√¥i .json
            
            if not json_files:
                return "NO_FILES"

            all_data = []
            seen_content = set() # D√πng ƒë·ªÉ l·ªçc tr√πng l·∫∑p gi·ªØa c√°c file
            
            # ƒê·ªçc t·ª´ng file v√† g·ªôp l·∫°i
            for file_name in json_files:
                try:
                    with open(file_name, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            for item in data:
                                content = item.get("content_text", "").strip()
                                # Ch·ªâ l·∫•y n·∫øu n·ªôi dung ch∆∞a t·ª´ng xu·∫•t hi·ªán v√† ƒë·ªß d√†i
                                if content and content not in seen_content and len(content) > 10:
                                    seen_content.add(content)
                                    # ƒê√°nh d·∫•u ngu·ªìn g·ªëc file ƒë·ªÉ d·ªÖ debug
                                    item["metadata"]["source_file"] = file_name 
                                    all_data.append(item)
                except:
                    pass # B·ªè qua file l·ªói

            if not all_data:
                return "EMPTY_DATA"

            # 3. Chu·∫©n b·ªã d·ªØ li·ªáu n·∫°p
            ids = []
            documents = []
            metadatas = []
            
            for item in all_data:
                # T·∫°o ID
                if "id" in item:
                    ids.append(str(item["id"]))
                else:
                    ids.append(str(uuid.uuid4()))
                
                documents.append(item.get("content_text", ""))
                
                # X·ª≠ l√Ω metadata
                meta = item.get("metadata", {}).copy()
                meta.update({
                    "url": item.get("url", ""),
                    "title": item.get("title", ""),
                    "hierarchy": item.get("hierarchy", ""),
                    "source_file": item.get("metadata", {}).get("source_file", "")
                })
                # L√†m s·∫°ch metadata (x√≥a None)
                clean_meta = {k: (str(v) if v is not None else "") for k, v in meta.items()}
                metadatas.append(clean_meta)
            
            # 4. N·∫°p Batch (40 items/l·∫ßn)
            batch_size = 40
            for i in range(0, len(ids), batch_size):
                collection.add(
                    ids=ids[i:i+batch_size],
                    documents=documents[i:i+batch_size],
                    metadatas=metadatas[i:i+batch_size]
                )
                
        return collection
        
    except Exception as e:
        print(f"L·ªói n·∫°p DB: {e}")
        return None

# --- G·ªåI H√ÄM N·∫†P V√Ä HI·ªÇN TH·ªä (B√äN NGO√ÄI CACHE) ---
with st.spinner("ƒêang qu√©t v√† t·ªïng h·ª£p d·ªØ li·ªáu t·ª´ t·∫•t c·∫£ c√°c file..."):
    collection = load_all_json_files()

# X·ª≠ l√Ω c√°c tr·∫°ng th√°i
if collection == "NO_FILES":
    st.error("‚ùå Kh√¥ng t√¨m th·∫•y file .json n√†o tr√™n GitHub.")
    st.stop()
elif collection == "EMPTY_DATA":
    st.error("‚ùå C√°c file JSON ƒë·ªÅu r·ªóng ho·∫∑c l·ªói ƒë·ªãnh d·∫°ng.")
    st.stop()
elif collection is None:
    st.error("‚ùå L·ªói h·ªá th·ªëng khi kh·ªüi t·∫°o ChromaDB.")
    st.stop()
else:
    # Hi·ªÉn th·ªã th√†nh c√¥ng
    count = collection.count()
    st.sidebar.success(f"üìö T·ªïng d·ªØ li·ªáu: **{count}** chunks")
    
    # Li·ªát k√™ c√°c file t√¨m th·∫•y (ƒë·ªÉ b·∫°n ki·ªÉm tra)
    with st.sidebar.expander("üìÇ C√°c file ƒë√£ ƒë·ªçc"):
        found_files = glob.glob("*.json")
        for f in found_files:
            st.write(f"- `{f}`")

# ================= 4. LOGIC RAG =================
def query_rag(query_text, model_name, top_k=12): # L·∫•y 12 chunk ƒë·ªÉ ƒë·ªß th√¥ng tin
    try:
        results = collection.query(
            query_texts=[query_text],
            n_results=top_k,
            include=["documents", "metadatas"]
        )
        
        context_parts = []
        sources = []
        
        if results["documents"]:
            for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
                h = meta.get('hierarchy', 'Th√¥ng tin')
                src_file = meta.get('source_file', 'unknown')
                
                context_parts.append(f"--- (File: {src_file}) | {h} ---\n{doc}")
                sources.append(f"- [{h}] (Ngu·ªìn: {src_file})")
                
        context = "\n\n".join(context_parts)
        
        prompt = f"""
        B·∫°n l√† tr·ª£ l√Ω ·∫£o h·ªó tr·ª£ ph√°p l√Ω. 
        D·ª±a v√†o c√°c vƒÉn b·∫£n ph√°p lu·∫≠t ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi.
        
        Y√äU C·∫¶U:
        1. Tr·∫£ l·ªùi chi ti·∫øt, c√≥ cƒÉn c·ª©.
        2. N·∫øu th√¥ng tin ƒë·∫øn t·ª´ nhi·ªÅu ngu·ªìn (nhi·ªÅu file), h√£y t·ªïng h·ª£p l·∫°i.
        3. VƒÉn phong r√µ r√†ng, d·ªÖ hi·ªÉu.
        
        NG·ªÆ C·∫¢NH D·ªÆ LI·ªÜU:
        {context}
        
        C√ÇU H·ªéI: {query_text}
        """
        
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(prompt)
        return response.text, list(set(sources)), context
        
    except Exception as e:
        return f"L·ªói: {str(e)}", [], ""

# ================= 5. GIAO DI·ªÜN CHAT =================
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Xin ch√†o! T√¥i ƒë√£ ƒë·ªçc h·∫øt c√°c file d·ªØ li·ªáu c·ªßa b·∫°n. B·∫°n c·∫ßn h·ªèi g√¨?"}]

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ƒêang t·ªïng h·ª£p th√¥ng tin..."):
            answer, sources, debug_ctx = query_rag(prompt, SELECTED_MODEL)
            
            if sources:
                full_resp = f"{answer}\n\n**üìö Ngu·ªìn tham kh·∫£o:**\n" + "\n".join(sources)
            else:
                full_resp = answer
            
            st.markdown(full_resp)
            
            with st.expander("üïµÔ∏è Xem d·ªØ li·ªáu t√¨m ƒë∆∞·ª£c"):
                st.text(debug_ctx)
            
            st.session_state.messages.append({"role": "assistant", "content": full_resp})
