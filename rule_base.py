# ================= 1. FIX L·ªñI SQLITE (B·∫ÆT BU·ªòC ƒê·ªÇ TR√äN C√ôNG) =================
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# =============================================================================

import streamlit as st
import json
import os
import uuid
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# ================= C·∫§U H√åNH TRANG =================
st.set_page_config(page_title="Chatbot H·ªó Tr·ª£ C∆∞ Tr√∫", layout="wide")
st.title("ü§ñ Chatbot T∆∞ V·∫•n Th·ªß T·ª•c C∆∞ Tr√∫")

# T√™n file d·ªØ li·ªáu chu·∫©n c·ªßa b·∫°n
JSON_FILE = "all_chunks_normalized.json" 
COLLECTION_NAME = "dichvucong_rag_final"

# ================= 2. C·∫§U H√åNH API & MODEL =================
st.sidebar.header("‚öôÔ∏è C·∫•u h√¨nh")

api_key = st.secrets.get("GEMINI_API_KEY") 
if not api_key:
    api_key = st.sidebar.text_input("Nh·∫≠p Google AI Studio API Key:", type="password")
    if not api_key:
        st.warning("üëâ Vui l√≤ng nh·∫≠p API Key ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
        st.stop()

genai.configure(api_key=api_key)

# T·ª± ƒë·ªông t√¨m model (∆Øu ti√™n Flash)
try:
    available_models = [m.name for m in genai.list_models() if 'gemini' in m.name and 'generateContent' in m.supported_generation_methods]
    default_idx = 0
    for i, m in enumerate(available_models):
        if "flash" in m: default_idx = i; break
    
    if available_models:
        SELECTED_MODEL = st.sidebar.selectbox("Ch·ªçn Model AI:", available_models, index=default_idx)
    else:
        st.sidebar.error("‚ùå Kh√¥ng t√¨m th·∫•y model Gemini n√†o.")
        st.stop()
except Exception as e:
    st.sidebar.error(f"L·ªói API: {e}")
    st.stop()

# ================= 3. H√ÄM LOAD D·ªÆ LI·ªÜU (KH√îNG C√ì UI B√äN TRONG) =================
@st.cache_resource(ttl="2h") 
def get_vector_collection():
    """
    H√†m n√†y ch·ªâ th·ª±c hi·ªán logic n·∫°p d·ªØ li·ªáu, tuy·ªát ƒë·ªëi KH√îNG v·∫Ω UI (st.write, st.progress...)
    ƒë·ªÉ tr√°nh l·ªói CacheReplayClosureError.
    """
    EMBEDDING_MODEL = "keepitreal/vietnamese-sbert"
    
    try:
        # 1. Kh·ªüi t·∫°o Client
        embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=EMBEDDING_MODEL
        )
        chroma_client = chromadb.Client()
        collection = chroma_client.get_or_create_collection(
            name=COLLECTION_NAME,
            embedding_function=embedding_function
        )
        
        # 2. Ki·ªÉm tra d·ªØ li·ªáu
        if collection.count() == 0:
            if not os.path.exists(JSON_FILE):
                return None # Tr·∫£ v·ªÅ None ƒë·ªÉ x·ª≠ l√Ω l·ªói ·ªü ngo√†i
            
            with open(JSON_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            if not data or not isinstance(data, list):
                return None

            # 3. Chu·∫©n b·ªã Batch
            ids = [str(uuid.uuid4()) if "id" not in item else str(item["id"]) for item in data]
            documents = [item.get("content_text", "") for item in data]
            metadatas = []
            
            for item in data:
                meta = item.get("metadata", {}).copy()
                meta.update({
                    "url": item.get("url", ""),
                    "title": item.get("title", ""),
                    "hierarchy": item.get("hierarchy", ""),
                })
                # X√≥a gi√° tr·ªã None ƒë·ªÉ tr√°nh l·ªói Chroma
                clean_meta = {k: (str(v) if v is not None else "") for k, v in meta.items()}
                metadatas.append(clean_meta)
            
            # 4. N·∫°p d·ªØ li·ªáu (Kh√¥ng d√πng progress bar ·ªü ƒë√¢y)
            batch_size = 40
            for i in range(0, len(ids), batch_size):
                collection.add(
                    ids=ids[i:i+batch_size],
                    documents=documents[i:i+batch_size],
                    metadatas=metadatas[i:i+batch_size]
                )
                
        return collection
        
    except Exception as e:
        print(f"L·ªói n·∫°p DB: {e}")
        return None

# --- G·ªåI H√ÄM N·∫†P D·ªÆ LI·ªÜU ---
# ƒê·∫∑t st.spinner ·ªü ngo√†i h√†m cache
with st.spinner("ƒêang kh·ªüi t·∫°o b·ªô nh·ªõ tri th·ª©c (L·∫ßn ƒë·∫ßu s·∫Ω m·∫•t kho·∫£ng 1-2 ph√∫t)..."):
    collection = get_vector_collection()

# X·ª≠ l√Ω tr∆∞·ªùng h·ª£p l·ªói
if collection is None:
    st.error(f"‚ùå Kh√¥ng th·ªÉ n·∫°p d·ªØ li·ªáu. Vui l√≤ng ki·ªÉm tra file `{JSON_FILE}` tr√™n GitHub.")
    st.stop()

# Hi·ªÉn th·ªã th·ªëng k√™
st.sidebar.success(f"üì¶ D·ªØ li·ªáu ƒë√£ n·∫°p: **{collection.count()}** chunks")

# ================= 4. LOGIC RAG =================
def query_rag(query_text, model_name, top_k=10):
    try:
        results = collection.query(
            query_texts=[query_text],
            n_results=top_k,
            include=["documents", "metadatas"]
        )
        
        context_parts = []
        sources = []
        
        if results["documents"]:
            for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
                h = meta.get('hierarchy', 'Th√¥ng tin')
                u = meta.get('url', '#')
                context_parts.append(f"--- M·ª§C: {h} ---\n{doc}")
                sources.append(f"- [{h}]({u})")
                
        context = "\n\n".join(context_parts)
        
        prompt = f"""
        B·∫°n l√† tr·ª£ l√Ω ·∫£o h√†nh ch√≠nh c√¥ng chuy√™n v·ªÅ Lu·∫≠t C∆∞ tr√∫.
        H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa c√¥ng d√¢n d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y.
        
        NGUY√äN T·∫ÆC:
        1. Tr·∫£ l·ªùi ch√≠nh x√°c, ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu.
        2. N·∫øu l√† quy tr√¨nh, h√£y li·ªát k√™ c√°c b∆∞·ªõc (B∆∞·ªõc 1, B∆∞·ªõc 2...).
        3. N·∫øu h·ªì s∆° y√™u c·∫ßu gi·∫•y t·ªù, h√£y li·ªát k√™ b·∫±ng g·∫°ch ƒë·∫ßu d√≤ng.
        4. Tuy·ªát ƒë·ªëi kh√¥ng b·ªãa ƒë·∫∑t th√¥ng tin n·∫øu kh√¥ng c√≥ trong ng·ªØ c·∫£nh.
        
        NG·ªÆ C·∫¢NH TH√îNG TIN:
        {context}
        
        C√ÇU H·ªéI: {query_text}
        """
        
        model = genai.GenerativeModel(model_name)
        response = model.generate_content(prompt)
        return response.text, list(set(sources)), context
        
    except Exception as e:
        return f"H·ªá th·ªëng ƒëang b·∫≠n: {str(e)}", [], ""

# ================= 5. GIAO DI·ªÜN CHAT =================
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Xin ch√†o! M√¨nh l√† tr·ª£ l√Ω ·∫£o h·ªó tr·ª£ th·ªß t·ª•c c∆∞ tr√∫ (Th∆∞·ªùng tr√∫, T·∫°m tr√∫, T√°ch h·ªô...). B·∫°n c·∫ßn gi√∫p g√¨ kh√¥ng?"}]

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ƒêang tra c·ª©u lu·∫≠t..."):
            answer, sources, debug_ctx = query_rag(prompt, SELECTED_MODEL)
            
            if sources:
                full_resp = f"{answer}\n\n**üìö Ngu·ªìn tham kh·∫£o:**\n" + "\n".join(sources)
            else:
                full_resp = answer
            
            st.markdown(full_resp)
            
            with st.expander("üïµÔ∏è Xem d·ªØ li·ªáu h·ªá th·ªëng t√¨m ƒë∆∞·ª£c"):
                st.text(debug_ctx)
            
            st.session_state.messages.append({"role": "assistant", "content": full_resp})
