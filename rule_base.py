# ================= FIX L·ªñI SQLITE TR√äN STREAMLIT CLOUD =================
# B·∫Øt bu·ªôc ph·∫£i ƒë·ªÉ 3 d√≤ng n√†y ·ªü tr√™n c√πng, tr∆∞·ªõc khi import chromadb
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# =======================================================================

import streamlit as st
import json
import os
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# ================= C·∫§U H√åNH =================
# T√™n file d·ªØ li·ªáu b·∫°n ƒë√£ upload l√™n GitHub
JSON_FILE = "all_procedures_normalized.json" 
COLLECTION_NAME = "dichvucong_rag"

# C·∫•u h√¨nh Page
st.set_page_config(page_title="Chatbot H·ªó Tr·ª£ C∆∞ Tr√∫", layout="centered")
st.title("ü§ñ Chatbot T∆∞ V·∫•n Th·ªß T·ª•c C∆∞ Tr√∫")

# ================= X·ª¨ L√ù API KEY =================
# ∆Øu ti√™n l·∫•y t·ª´ Secrets, n·∫øu kh√¥ng c√≥ th√¨ hi·ªán √¥ nh·∫≠p
api_key = st.secrets.get("GEMINI_API_KEY") 

if not api_key:
    api_key = st.text_input("Nh·∫≠p Google AI Studio API Key:", type="password")
    if not api_key:
        st.info("üëâ Vui l√≤ng nh·∫≠p API Key ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
        st.stop()

genai.configure(api_key=api_key)

# ================= H√ÄM LOAD D·ªÆ LI·ªÜU & VECTOR DB =================
@st.cache_resource
def initialize_vector_db():
    # S·ª≠ d·ª•ng model nh·∫π h∆°n ƒë·ªÉ tr√°nh b·ªã s·∫≠p (Out of Memory) tr√™n Cloud Free
    # N·∫øu mu·ªën d√πng BAAI/bge-m3 m√† b·ªã l·ªói restart app, h√£y ƒë·ªïi d√≤ng d∆∞·ªõi th√†nh: "keepitreal/vietnamese-sbert"
    EMBEDDING_MODEL = "BAAI/bge-m3" 
    
    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name=EMBEDDING_MODEL
    )
    
    # D√πng Client ephermeral (ch·∫°y tr√™n RAM)
    chroma_client = chromadb.Client()
    
    try:
        collection = chroma_client.get_or_create_collection(
            name=COLLECTION_NAME,
            embedding_function=embedding_function
        )
        
        # Ch·ªâ n·∫°p d·ªØ li·ªáu n·∫øu Collection ƒëang r·ªóng
        if collection.count() == 0:
            if not os.path.exists(JSON_FILE):
                st.error(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file: {JSON_FILE}. H√£y upload file n√†y l√™n GitHub c√πng ch·ªó v·ªõi app.py")
                return None
                
            with st.spinner("ƒêang n·∫°p d·ªØ li·ªáu l·∫ßn ƒë·∫ßu (c√≥ th·ªÉ m·∫•t 1-2 ph√∫t)..."):
                with open(JSON_FILE, "r", encoding="utf-8") as f:
                    data = json.load(f)
                
                # Batch processing ƒë·ªÉ n·∫°p nhanh h∆°n
                ids = []
                documents = []
                metadatas = []
                
                for item in data:
                    ids.append(item["id"])
                    documents.append(item["content_text"])
                    
                    # X·ª≠ l√Ω metadata an to√†n
                    meta = item.get("metadata", {}).copy()
                    meta.update({
                        "url": item.get("url", ""),
                        "title": item.get("title", ""),
                        "hierarchy": item.get("hierarchy", ""),
                    })
                    # X√≥a gi√° tr·ªã None ƒë·ªÉ tr√°nh l·ªói Chroma
                    clean_meta = {k: (v if v is not None else "") for k, v in meta.items()}
                    metadatas.append(clean_meta)
                
                # N·∫°p theo l√¥ 100 item/l·∫ßn
                batch_size = 100
                total_batches = len(ids) // batch_size + 1
                progress_bar = st.progress(0)
                
                for i in range(0, len(ids), batch_size):
                    collection.add(
                        ids=ids[i:i+batch_size],
                        documents=documents[i:i+batch_size],
                        metadatas=metadatas[i:i+batch_size]
                    )
                    # C·∫≠p nh·∫≠t thanh ti·∫øn tr√¨nh
                    current_progress = min((i + batch_size) / len(ids), 1.0)
                    progress_bar.progress(current_progress)
                
                progress_bar.empty() # X√≥a thanh ti·∫øn tr√¨nh khi xong
                
        return collection
        
    except Exception as e:
        st.error(f"L·ªói kh·ªüi t·∫°o DB: {str(e)}")
        return None

# G·ªçi h√†m kh·ªüi t·∫°o
collection = initialize_vector_db()

if not collection:
    st.stop()

# ================= LOGIC RAG & CHAT =================
def query_rag(query_text, top_k=3):
    try:
        results = collection.query(
            query_texts=[query_text],
            n_results=top_k,
            include=["documents", "metadatas"]
        )
        
        context_parts = []
        sources = []
        
        if results["documents"]:
            for doc, meta in zip(results["documents"][0], results["metadatas"][0]):
                hierarchy = meta.get('hierarchy', meta.get('title', 'Th√¥ng tin'))
                url = meta.get('url', '#')
                context_parts.append(f"[{hierarchy}]\n{doc}")
                sources.append(f"- [{hierarchy}]({url})")
                
        context = "\n\n".join(context_parts)
        
        prompt = f"""
        B·∫°n l√† tr·ª£ l√Ω ·∫£o h√†nh ch√≠nh c√¥ng. H√£y tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin sau:
        
        NG·ªÆ C·∫¢NH:
        {context}
        
        C√ÇU H·ªéI: {query_text}
        
        Y√äU C·∫¶U: Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c b·∫±ng ti·∫øng Vi·ªát. N·∫øu kh√¥ng c√≥ th√¥ng tin, h√£y n√≥i kh√¥ng bi·∫øt.
        """
        
        model = genai.GenerativeModel('gemini-1.5-flash')
        response = model.generate_content(prompt)
        return response.text, sources
    except Exception as e:
        return f"Xin l·ªói, h·ªá th·ªëng ƒëang b·∫≠n. L·ªói: {str(e)}", []

# ================= GIAO DI·ªÜN CHAT =================
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Xin ch√†o! B·∫°n c·∫ßn t√¨m hi·ªÉu v·ªÅ th·ªß t·ª•c c∆∞ tr√∫ n√†o?"}]

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ƒêang suy nghƒ©..."):
            answer, sources = query_rag(prompt)
            
            # X·ª≠ l√Ω ngu·ªìn tr√πng l·∫∑p
            unique_sources = list(set(sources))
            
            if unique_sources:
                full_response = f"{answer}\n\n**Ngu·ªìn tham kh·∫£o:**\n" + "\n".join(unique_sources)
            else:
                full_response = answer
                
            st.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})
